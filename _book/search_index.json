[["index.html", "Network-Based Metabolomics in R: Data Preprocessing, Co-expression Network Analysis, and MetaboAnalyst Workflows About", " Network-Based Metabolomics in R: Data Preprocessing, Co-expression Network Analysis, and MetaboAnalyst Workflows Menna Arafat 2025-08-24 About This book presents a comprehensive, R-based pipeline for metabolomics data analysis, from data preprocessing and normalization to network construction, visualization, and functional enrichment analysis. By integrating state-of-the-art tools such as WGCNA and MetaboAnalyst, the workflow enables systematic exploration of metabolomics datasets to uncover biologically meaningful patterns. The pipeline has been applied in our published study: Ramzy, A., Abdelmoneim, T.K., Arafat, M., et al. (2025). Metabolomic analysis reveals key changes in amino acid metabolism in colorectal cancer patients. Amino Acids, 57, 22. https://doi.org/10.1007/s00726-025-03448-3 Acknowledgments This pipeline builds upon the previous work of the bioinformaticians at the Proteomics Lab, 57357 Children’s Cancer Hospital Egypt (CCHE): Mohamed Soudy, Abdelrahman Atef, Ali Mostafa, Ahmed Karam, and Menna Arafat, as well as publicly available resources. References Horvath, S. (2011). Weighted network analysis: Applications in genomics and systems biology. Springer. https://doi.org/10.1007/978-1-4419-8819-5 Xia Lab. (2025, August 9). MetaboAnalystR Tutorial. MetaboAnalyst – MetaboAnalystR 4.0 Documentation. Retrieved August 24, 2025, from https://www.metaboanalyst.ca/docs/RTutorial.xhtml "],["metabolomic-raw-data-processing.html", "Section 1 Metabolomic Raw data Processing 1.1 Internal Functions 1.2 Wrapper Function", " Section 1 Metabolomic Raw data Processing Description: This R script performs a complete preprocessing pipeline for untargeted metabolomics data. It handles normalization, error correction, missing value imputation, and extraction of group-specific metabolites. Main Steps: 0. Load Data: Loads raw metabolite data from .xlsx files and HMDB annotation file to map IDs to metabolite names. 1. Normalization: Applies Probabilistic Quotient Normalization (PQN) to correct for sample variability. 2. Error PPM Filtering: Replaces measurements with PPM errors outside the range (-10, 10) with NA. 3. Deduplication: Retains only the highest intensity value for duplicated metabolites per sample. 4. HMDB mapping Map HMDB ids to Metabolite names using mapping file from Human Metabolome Database (HMDB). 5. Filtering: Keeps metabolites present in, at least, 60% of samples per group. 6. Get Unique Metabolites: Merges datasets for both groups for downstream analysis and Identifies metabolites unique to each group. 7. Imputation: Fills missing values using random noise around the group median. (median +/- 0.01*median) Project Initialization mypath= &quot;C:/Users/USER/Documents/Github/CRC_project/&quot; #setwd(mypath) # Uncomment the following commands dir.create(&quot;output&quot;) dir.create(&quot;plots&quot;) dir.create(&quot;input&quot;) #Load libraries library(tibble) library(plyr) library(dplyr) library(tidyverse) library(openxlsx) library(DT) Set your variables #Set group distribution group_names= c(&quot;CRC&quot;, &quot;Ctrl&quot;) maindir= &quot;C:/Users/USER/Documents/Github/CRC_project/&quot; mapping_file= read.csv(&quot;input/Metabolites-HMDB.csv&quot;) 1.1 Internal Functions #[[0]] load data load_raw= function(maindir){ raw_list=list() for (i in list.files(paste0(maindir, &quot;input&quot;), pattern = &quot;Gnp&quot;) ){ raw_list[[i]]= read.xlsx(paste0(maindir,&quot;input/&quot;, i)) } return(raw_list) } # Internal Functions # Normalization and error filtration #[[1]] PQN (Probabilistic Quotient Normalization) pqn = function(X, reference = NULL) { X = mutate_all(X, function(x) as.numeric(as.character(x))) X[X == 0] = NA # If no reference is supplied, use the QC sample (with highest total intensity) if (is.null(reference)) { reference = X[, names(which.max(colSums(X, na.rm = TRUE)))] } # Apply PQN logic: sample / median(sample / reference) # Dividing each sample by the median of feature-wise ratios, X.norm = as.data.frame(apply(X, 2, function(sample) { # old labpqn #scale_factor = median(as.numeric(sample) / median(as.numeric(reference),na.rm = TRUE) , na.rm = TRUE) scale_factor = median(as.numeric(sample) / as.numeric(reference), na.rm = TRUE) sample / scale_factor })) return(X.norm) } # Normalize intensities and remove intensities for the exceeding error ppm normalize= function(raw_list, metabolite_col=&quot;Metabolite.name&quot; ){ data_norm= list() for(i in seq_along(raw_list)){ df= raw_list[[i]] ID_idx= which(grepl(metabolite_col, colnames(df))) assign(&quot;ID_idx&quot;, ID_idx, envir = .GlobalEnv) sample_idx= which(grepl(&quot;Sample&quot;, df[1,]) &amp; !grepl(&quot;_Sample_&quot;, df[1,])) assign(&quot;sample_idx&quot;, sample_idx, envir = .GlobalEnv) error_idx = which(grepl(&quot;ERROR&quot;, df[1,])) assign(&quot;error_idx&quot;, error_idx, envir = .GlobalEnv) df= df[-c(1,2), ] # remove extra rows # Apply PQN to sample intensities df[, sample_idx ]= pqn(df[, sample_idx ] ) colnames(df) = gsub(&quot;\\\\s+&quot;, &quot;.&quot;, colnames(df)) colnames(df)= gsub(&quot;.*-&quot;, &quot;&quot;, colnames(df) ) data_norm[[i]] = df } # combine both modes all.data= do.call( rbind, data_norm) write.csv(all.data, &quot;output/data_normalized.csv&quot;, row.names = F) print(&#39;Normalization is done&#39;) return(all.data) } #[[2]] Error PPM filteration beyond the range of (-10, 10) error_ppm_filter = function(df, error_idx) { for (i in error_idx) { for (j in 1:nrow(df)) { col_index = i df[j, col_index] = ifelse( between(as.numeric(df[j, col_index]), -10, 10), as.numeric(df[j, col_index]), NA ) if (is.na(df[j, col_index])) { df[j, col_index + 1] = NA } } } return(df) } # [[3]] Deduplication per sample #the reason we do so is to keep the highest intensity for the duplicated metabolite for that sample sample_deduplicate= function(all.count){ colnames(all.count)[1]= &quot;ID&quot; # Initialize the output df_unduplicated = data.frame(ID= unique(all.count$ID)) for (i in 2:ncol(all.count)) { df = all.count[, c(1, i)] sample.name= colnames(all.count)[i] colnames(df) = c(&quot;ID&quot;, sample.name ) # Group by ID and retain the row with the max Value df_undupl = df |&gt; arrange(desc(!!sym(sample.name))) |&gt; distinct(ID, .keep_all = TRUE) # left_join: keeping all rows from the left (first) data frame and adding matching columns from the right (second) data frame. df_unduplicated = left_join(df_unduplicated, df_undupl, by = &quot;ID&quot;) # Drop rows where all columns are NA df_unduplicated = df_unduplicated[!apply(is.na(df_unduplicated), 1, all), ] } print(&quot;De-Duplication is done&quot;) write.csv(df_unduplicated, &quot;output/data_sample_deduplicated.csv&quot;, row.names = F) return(df_unduplicated) } #[[4]] Map HMID to metabolites name map_id= function(id, mapping_file){ mapped_id= mapping_file$Name[match(id, mapping_file$HMDB_ID)] |&gt; as.data.frame() names(mapped_id)= &quot;ID&quot; return(mapped_id) } # Apply # mapped_ids= map_id(df_deduplicated$ID, mapping_file) # df_mapped= cbind(mapped_ids, df_deduplicated[,-1]) # df_mapped= df_mapped[!is.na(df_mapped$ID), ] # deduplicate similar mapped ids deduplicate = function(exp) { exp= as.data.frame(exp) row.names(exp)= NULL colnames(exp)[1]= &quot;ID&quot; # Check for duplicated IDs if (anyDuplicated(exp$ID) &gt; 0) { # Convert all columns (except ID) to numeric exp[,-1] = lapply(exp[,-1], as.numeric) exp = exp %&gt;% dplyr::mutate(mean_expr = rowMeans(dplyr::select(., where(is.numeric)))) %&gt;% arrange(desc(mean_expr)) |&gt; distinct(ID, .keep_all = TRUE) |&gt; dplyr::select(-mean_expr) } return(exp) } get_group_dist= function(df, group_names){ # list of group distribution group_dist=list() for(i in group_names){ group_dist[[i]]= which(grepl(i, colnames(df))) } names(group_dist)= group_names return(group_dist) } #[[5]] Filtration of metabolites missing in 50 % of samples per group filter_missing = function(df, group_dist, cut_off ){ result = lapply(group_dist, function(gcols){ subdf = df[, c(1,gcols), drop = FALSE] keep = apply(subdf, 1, function(x) sum(is.na(x)) &lt;= (1 - cut_off) * (ncol(subdf)-1)) subdf[keep, ] }) # merge 2 datasets df_shared= Reduce(function(x, y) inner_join(x, y, by = &quot;ID&quot;), result) write.csv(df_shared, &quot;output/data_shared_not_imputed.csv&quot;, row.names = F) return(df_shared) } #[[6]] get_uniques get_uniques= function(df, group_dist ){ result = lapply(group_dist, function(gcols){ subdf = df[, c(1,gcols), drop = FALSE] all_missings= apply(subdf, 1, function(x) sum(is.na(x)) == length(gcols)) subdf[all_missings,] }) # Uniques uniques = lapply(seq_along(result), function(i) { current_missing = result[[i]] other_dfs = result[-i] # Combine all IDs from other data frames other_ids = bind_rows(other_dfs) %&gt;% distinct(ID) # Keep only rows that are missing in other groups and not found missing in my current_missing # These are my uniques setdiff(other_ids$ID, current_missing$ID) }) group_names= names(group_dist) # combine uniques names(uniques)= group_names uniques_l= stack(uniques) names(uniques_l)= c(&quot;ID&quot;, &quot;group&quot;) unique_df= inner_join(uniques_l, df, by=&quot;ID&quot;) write.csv(unique_df, &quot;output/data_uniques.csv&quot;, row.names = F) return(unique_df) } #[[7]] Imputation impute_me = function(df, group_dist, range = 0.01) { for (name in names(group_dist)) { gcols= group_dist[[name]] na_rows = which(rowSums(is.na(df[, gcols])) &gt; 0) row_meds = apply(df[na_rows, gcols, drop = FALSE], 1, function(m){ m = as.numeric(m) med = median(m, na.rm = TRUE) runif(1, min = med - range, max = med + range) }) df[na_rows, gcols] = row_meds } write.csv(df, paste0(mypath,&quot;output/data_for_downstream.csv&quot;), row.names = F) write.csv(df, paste0(mypath,&quot;input/data_for_downstream.csv&quot;), row.names = F) return(df) } 1.2 Wrapper Function # Wrapping function process_raw_metab= function(maindir, group_names, mapping_file, cut_off=.6 ){ #load data raw_list= load_raw(maindir) # Apply PQN normalization for each mode and filtration all.count= normalize(raw_list) # error ppm filter all.count[, error_idx]= as.data.frame(lapply(all.count[, error_idx], as.numeric) ) filtered.data= error_ppm_filter(all.count, error_idx=error_idx) error.filtered= filtered.data[, c(ID_idx, sample_idx)] # Deduplication per sample df_deduplicated= sample_deduplicate(error.filtered) # ID mapping mapped_ids= map_id(df_deduplicated$ID, mapping_file) df_mapped= cbind(mapped_ids, df_deduplicated[,-1]) df_mapped= df_mapped[!is.na(df_mapped$ID), ] # deduplicate similar mapped ids df_mapped_de= deduplicate(df_mapped) # get group distribution group_dist=get_group_dist(df_deduplicated, group_names ) # filterout metabolites missing in 40% and keep shared df_shared= filter_missing(df_mapped_de, group_dist, cut_off) # get uniques df_uniques= get_uniques(df_mapped_de, group_dist) # impute missings df_imputed= impute_me(df_shared, group_dist) print(&quot;All is Done!&quot;) return(df_imputed) } Apply Main Function output= process_raw_metab(maindir, group_names, mapping_file, cut_off= .6) ## [1] &quot;Normalization is done&quot; ## [1] &quot;De-Duplication is done&quot; ## [1] &quot;All is Done!&quot; datatable(output, options = list(scrollX=TRUE, scrollY=&quot;600px&quot;, autoWidth = TRUE)) ## R version 4.4.1 (2024-06-14 ucrt) ## Platform: x86_64-w64-mingw32/x64 ## Running under: Windows 10 x64 (build 19045) ## ## Matrix products: default ## ## ## locale: ## [1] LC_COLLATE=English_United States.utf8 ## [2] LC_CTYPE=English_United States.utf8 ## [3] LC_MONETARY=English_United States.utf8 ## [4] LC_NUMERIC=C ## [5] LC_TIME=English_United States.utf8 ## ## time zone: Africa/Cairo ## tzcode source: internal ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] DT_0.33 openxlsx_4.2.6.1 lubridate_1.9.3 forcats_1.0.0 ## [5] stringr_1.5.1 purrr_1.0.2 readr_2.1.5 tidyr_1.3.1 ## [9] ggplot2_3.5.1 tidyverse_2.0.0 dplyr_1.1.4 plyr_1.8.9 ## [13] tibble_3.2.1 ## ## loaded via a namespace (and not attached): ## [1] gtable_0.3.5 jsonlite_1.8.8 compiler_4.4.1 zip_2.3.1 ## [5] tidyselect_1.2.1 Rcpp_1.0.13 jquerylib_0.1.4 scales_1.3.0 ## [9] yaml_2.3.10 fastmap_1.2.0 R6_2.5.1 generics_0.1.3 ## [13] knitr_1.48 htmlwidgets_1.6.4 bookdown_0.40 munsell_0.5.1 ## [17] tzdb_0.4.0 bslib_0.8.0 pillar_1.11.0 rlang_1.1.4 ## [21] stringi_1.8.4 cachem_1.1.0 xfun_0.46 sass_0.4.9 ## [25] timechange_0.3.0 cli_3.6.3 withr_3.0.0 magrittr_2.0.3 ## [29] crosstalk_1.2.1 digest_0.6.36 grid_4.4.1 rstudioapi_0.16.0 ## [33] hms_1.1.3 lifecycle_1.0.4 vctrs_0.6.5 evaluate_0.24.0 ## [37] glue_1.7.0 colorspace_2.1-1 rmarkdown_2.27 tools_4.4.1 ## [41] pkgconfig_2.0.3 htmltools_0.5.8.1 "],["dimensionality-reduction-techniques.html", "Section 2 Dimensionality Reduction Techniques 2.1 Input data Preparation for Metaboanalyst 2.2 PCA 2.3 PLS-DA 2.4 splsda", " Section 2 Dimensionality Reduction Techniques Description: This pipeline performs variuos dimensionality reduction techniques: PCA and PLS-DA and sPLS-DA- using Metaboanalyst R package. Project Initialization: mypath= &quot;C:/Users/USER/Documents/Github/CRC_project/&quot; # Uncomment the following commands dir.create(&quot;output&quot;) dir.create(&quot;plots&quot;) dir.create(&quot;input&quot;) #Load libraries library(tibble) library(plyr) library(dplyr) library(tidyverse) library(openxlsx) library(cowplot) library(ggplot2) library(&quot;MetaboAnalystR&quot;) library(DT) #BiocManager::install(&quot;DT&quot;) #load data data= read.csv(paste0(mypath,&quot;input/data_for_downstream.csv&quot;)) data = data |&gt; column_to_rownames(colnames(data)[1]) 2.1 Input data Preparation for Metaboanalyst #Specify the group pattern in your samples and add it as the first row in the sheet. group_dist= gsub(&quot;_.*&quot;, &quot;&quot;, colnames(data)) group_levels= unique(group_dist) print(group_dist) ## [1] &quot;CRC&quot; &quot;CRC&quot; &quot;CRC&quot; &quot;CRC&quot; &quot;CRC&quot; &quot;CRC&quot; &quot;CRC&quot; &quot;CRC&quot; &quot;CRC&quot; &quot;CRC&quot; ## [11] &quot;Ctrl&quot; &quot;Ctrl&quot; &quot;Ctrl&quot; &quot;Ctrl&quot; &quot;Ctrl&quot; &quot;Ctrl&quot; &quot;Ctrl&quot; &quot;Ctrl&quot; &quot;Ctrl&quot; &quot;Ctrl&quot; data_formatted= rbind(group_dist, data) datatable(data_formatted, options = list(scrollX=TRUE, scrollY=&quot;600px&quot;, autoWidth = TRUE)) write.csv(data_formatted, paste0(mypath,&quot;output/for_metaboanalyst.csv&quot;)) # Intiate metaboanalyst object mSet&lt;-InitDataObjects(&quot;pktable&quot;, &quot;stat&quot;, FALSE) ## Starting Rserve... ## &quot;C:\\Users\\USER\\AppData\\Local\\R\\WIN-LI~1\\4.4\\Rserve\\libs\\x64\\Rserve.exe&quot; --no-save ## [1] &quot;MetaboAnalyst R objects initialized ...&quot; mSet&lt;-Read.TextData(mSet, paste0(mypath,&quot;output/for_metaboanalyst.csv&quot;), &quot;colu&quot;, &quot;disc&quot;) mSet&lt;-SanityCheckData(mSet) ## [1] &quot;Successfully passed sanity check!&quot; ## [2] &quot;Samples are not paired.&quot; ## [3] &quot;2 groups were detected in samples.&quot; ## [4] &quot;Only English letters, numbers, underscore, hyphen and forward slash (/) are allowed.&quot; ## [5] &quot;&lt;font color=\\&quot;orange\\&quot;&gt;Other special characters or punctuations (if any) will be stripped off.&lt;/font&gt;&quot; ## [6] &quot;All data values are numeric.&quot; ## [7] &quot;A total of 0 (0%) missing values were detected.&quot; ## [8] &quot;&lt;u&gt;By default, missing values will be replaced by 1/5 of min positive values of their corresponding variables&lt;/u&gt;&quot; ## [9] &quot;Click the &lt;b&gt;Proceed&lt;/b&gt; button if you accept the default practice;&quot; ## [10] &quot;Or click the &lt;b&gt;Missing Values&lt;/b&gt; button to use other methods.&quot; mSet&lt;-ReplaceMin(mSet) mSet&lt;-PreparePrenormData(mSet) mSet&lt;-Normalization(mSet, &quot;NULL&quot;, &quot;NULL&quot;, &quot;AutoNorm&quot;, ratio=FALSE) 2.2 PCA Principal Component Analysis (PCA) is a dimensionality reduction technique used to explore patterns in high-dimensional data. It transforms the original variables into a smaller set of uncorrelated variables called principal components, which capture the maximum variance in the data. PCA is often used for data visualization, outlier detection, and identifying natural clusters or trends without any prior knowledge of group labels and so it is unsupervised! mSet&lt;-PCA.Anal(mSet) mSet&lt;-PlotPCA2DScore(mSet, paste0(mypath,&quot;plots/PCA_2D&quot;), &quot;png&quot;, 150, width=NA, 1,2,0.95,0,0) PCA_data &lt;- mSet$analSet$pca$x write.csv(PCA_data, paste0(mypath,&quot;output/pca_score.csv&quot;) , row.names = TRUE) PCA plot # Load PCA data with samples as row names PCA_data &lt;- read.csv(paste0(mypath, &quot;output/pca_score.csv&quot;), row.names = 1) # Calculate variance explained by each principal component vars &lt;- apply(PCA_data, 2, var) PC1_var &lt;- round((var(PCA_data$PC1) / sum(vars)) * 100, 1) PC2_var &lt;- round((var(PCA_data$PC2) / sum(vars)) * 100, 1) PC3_var &lt;- round((var(PCA_data$PC3) / sum(vars)) * 100, 1) # Add group metadata PCA_data$group = group_dist group_colors &lt;- c(&quot;#fc8d62&quot;, &quot;#66c2a5&quot;) names(group_colors) &lt;- group_levels pca_plot= ggplot(PCA_data, aes(x = PC1, y = PC2, colour = group)) + stat_ellipse(aes(fill = group), level = 0.95, geom = &quot;polygon&quot;, alpha = 0.1, colour = NA) + geom_point(aes(fill = group), alpha = 0.7, shape = 21, size = 4, colour = &quot;black&quot;, stroke = 1.5) + scale_fill_manual(values = group_colors) + scale_color_manual(values = group_colors) + labs(color = &quot;Groups&quot;, fill = &quot;Groups&quot;) + xlab(paste0(&quot;PC1 (&quot;, PC1_var, &quot;%)&quot;)) + ylab(paste0(&quot;PC2 (&quot;, PC2_var, &quot;%)&quot;)) + theme( #legend.background = element_rect(size = 0.5, linetype = &quot;solid&quot;, colour = &quot;black&quot;), legend.title = element_text(face = &quot;bold&quot;, size = 14), legend.position = &quot;right&quot;, text = element_text(size = 16, face = &quot;bold&quot;)) + theme( plot.background = element_rect(fill = &quot;white&quot;, color = NA), panel.background = element_rect(fill = &quot;white&quot;, color = NA), panel.grid = element_blank(), axis.line = element_line(color = &quot;black&quot;), axis.ticks = element_line(color = &quot;black&quot;), text = element_text(face = &quot;bold&quot;), plot.title = element_text(hjust = 0.5)) print(pca_plot) ggsave(paste0(mypath,&quot;plots/PCA_plot.png&quot;), plot = pca_plot, dpi = 600, width = 10, height = 7) 2.3 PLS-DA Partial Least Squares Discriminant Analysis (PLS-DA) is a supervised method that combines PLS regression with classification. PLS-DA models the relationship between predictors (e.g., gene or metabolite expression) and a categorical outcome (e.g., disease status), maximizing the covariance between them. And so It does find directions in the data that best separate predefined groups or classes. library(pls) # call adjusted function for VIP plot source(&quot;C:/Users/USER/Documents/Rscripts/All_Functions/plot_vip.R&quot;) mSet&lt;-PLSR.Anal(mSet, reg=TRUE) mSet&lt;-PlotPLSPairSummary(mSet, paste0(mypath, &quot;plots/pls_pair&quot;), &quot;png&quot;, 72, width=NA, 5) mSet&lt;-PlotPLS2DScore(mSet, paste0(mypath,&quot;plots/pls_score2d&quot;), &quot;png&quot;, 72, width=NA, 1,2,0.95,0,0) mSet&lt;-PlotPLS3DScoreImg(mSet, paste0(mypath,&quot;plots/pls_score3d&quot;), &quot;png&quot;, 72, width=NA, 1,2,3, 40) mSet&lt;-PlotPLSLoading(mSet, paste0(mypath,&quot;plots/pls_loading&quot;), &quot;png&quot;, 72, width=NA, 1, 2); mSet&lt;-PLSDA.CV(mSet, &quot;T&quot;,5, &quot;Q2&quot;) mSet&lt;-PlotPLS.Classification(mSet, paste0(mypath,&quot;plots/pls_cv&quot;), &quot;png&quot;, 72, width=NA) mSet&lt;-PlotPLS.Imp(mSet, paste0(mypath,&quot;plots/pls_imp&quot;), &quot;png&quot;, 72, width=7, &quot;vip&quot;, &quot;Comp. 1&quot;, 15, FALSE) mSet&lt;-PLSDA.Permut(mSet, 100, &quot;accu&quot;) ## [1] &quot;performing 100 permutations ...&quot; ## [1] &quot;Empirical p value: p = 1 (100/100)&quot; mSet&lt;-PlotPLS.Permutation(mSet, paste0(mypath,&quot;plots/pls_perm&quot;), &quot;png&quot;, 72, width=NA) VIP plot for displaying top features discriminating the two groups p= Plotvip(mSetObj = mSet,feat.num=15, color.BW =FALSE) print(p) ## $mar ## [1] 5 9 3 6 # Save png(paste0(mypath,&quot;plots/vip_plot2.png&quot;), unit = &quot;in&quot;,res = 600, width = 7, height = 7) Plotvip(mSetObj = mSet,feat.num=15, color.BW =FALSE) dev.off() ## png ## 2 2.4 splsda Sparse PLS-DA (sPLS-DA) extends PLS-DA by introducing variable selection through sparsity constraints. It separates classes while identifying a minimal set of discriminative features. # # Perform sPLS-DA analysis # mSet&lt;-SPLSR.Anal(mSet, 1, 1, &quot;same&quot;, &quot;Mfold&quot;) # # Plot sPLS-DA overview # mSet&lt;-PlotSPLSPairSummary(mSet, paste0(mypath,&quot;plots/spls_pair&quot;), format = &quot;png&quot;, dpi=72, width=NA, 5) # # Create 2D sPLS-DA Score Plot # mSet&lt;-PlotSPLS2DScore(mSet, paste0(mypath,&quot;plots/spls_score2d&quot;), format = &quot;png&quot;, dpi=72, width=NA, 1, 2, 0.95, 1, 0) # # Create 3D sPLS-DA Score Plot # mSet&lt;-PlotSPLS3DScoreImg(mSet, paste0(mypath,&quot;plots/spls_score3d&quot;), format = &quot;png&quot;, 72, width=NA, 1, 2, 3, 40) # # Create sPLS-DA loadings plot # mSet&lt;-PlotSPLSLoading(mSet, paste0(mypath,&quot;plots/spls_loading&quot;), format = &quot;png&quot;, dpi=72, width=NA, 1,&quot;overview&quot;) # # Perform cross-validation and plot sPLS-DA classification # mSet&lt;-PlotSPLSDA.Classification(mSet, paste0(mypath,&quot;plots/spls_cv&quot;), format = &quot;png&quot;, dpi=72, width=NA) ## R version 4.4.1 (2024-06-14 ucrt) ## Platform: x86_64-w64-mingw32/x64 ## Running under: Windows 10 x64 (build 19045) ## ## Matrix products: default ## ## ## locale: ## [1] LC_COLLATE=English_United States.utf8 ## [2] LC_CTYPE=English_United States.utf8 ## [3] LC_MONETARY=English_United States.utf8 ## [4] LC_NUMERIC=C ## [5] LC_TIME=English_United States.utf8 ## ## time zone: Africa/Cairo ## tzcode source: internal ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] caret_6.0-94 lattice_0.22-6 pls_2.8-3 ## [4] Rserve_1.8-13 MetaboAnalystR_3.2.0 cowplot_1.1.3 ## [7] DT_0.33 openxlsx_4.2.6.1 lubridate_1.9.3 ## [10] forcats_1.0.0 stringr_1.5.1 purrr_1.0.2 ## [13] readr_2.1.5 tidyr_1.3.1 ggplot2_3.5.1 ## [16] tidyverse_2.0.0 dplyr_1.1.4 plyr_1.8.9 ## [19] tibble_3.2.1 ## ## loaded via a namespace (and not attached): ## [1] RColorBrewer_1.1-3 rstudioapi_0.16.0 jsonlite_1.8.8 ## [4] magrittr_2.0.3 farver_2.1.2 rmarkdown_2.27 ## [7] ragg_1.3.2 vctrs_0.6.5 multtest_2.61.0 ## [10] memoise_2.0.1 Cairo_1.6-2 htmltools_0.5.8.1 ## [13] pROC_1.18.5 sass_0.4.9 parallelly_1.38.0 ## [16] KernSmooth_2.23-24 bslib_0.8.0 htmlwidgets_1.6.4 ## [19] impute_1.79.0 plotly_4.10.4 cachem_1.1.0 ## [22] igraph_2.0.3 lifecycle_1.0.4 iterators_1.0.14 ## [25] pkgconfig_2.0.3 Matrix_1.7-0 R6_2.5.1 ## [28] fastmap_1.2.0 future_1.33.2 digest_0.6.36 ## [31] pcaMethods_1.97.0 colorspace_2.1-1 siggenes_1.79.0 ## [34] textshaping_0.4.0 crosstalk_1.2.1 ellipse_0.5.0 ## [37] RSQLite_2.3.7 labeling_0.4.3 timechange_0.3.0 ## [40] httr_1.4.7 compiler_4.4.1 proxy_0.4-27 ## [43] bit64_4.0.5 withr_3.0.0 glasso_1.11 ## [46] BiocParallel_1.39.0 DBI_1.2.3 qs_0.26.3 ## [49] highr_0.11 gplots_3.1.3.1 MASS_7.3-60.2 ## [52] lava_1.8.0 gtools_3.9.5 caTools_1.18.2 ## [55] ModelMetrics_1.2.2.2 tools_4.4.1 zip_2.3.1 ## [58] future.apply_1.11.2 nnet_7.3-19 glue_1.7.0 ## [61] nlme_3.1-164 grid_4.4.1 reshape2_1.4.4 ## [64] fgsea_1.31.0 generics_0.1.3 recipes_1.1.0 ## [67] gtable_0.3.5 tzdb_0.4.0 class_7.3-22 ## [70] data.table_1.15.4 RApiSerialize_0.1.3 hms_1.1.3 ## [73] stringfish_0.16.0 BiocGenerics_0.52.0 foreach_1.5.2 ## [76] pillar_1.11.0 limma_3.61.5 splines_4.4.1 ## [79] survival_3.6-4 bit_4.0.5 tidyselect_1.2.1 ## [82] locfit_1.5-9.10 knitr_1.48 bookdown_0.40 ## [85] edgeR_4.3.5 stats4_4.4.1 xfun_0.46 ## [88] Biobase_2.64.0 scrime_1.3.5 statmod_1.5.0 ## [91] hardhat_1.4.0 timeDate_4032.109 stringi_1.8.4 ## [94] lazyeval_0.2.2 yaml_2.3.10 evaluate_0.24.0 ## [97] codetools_0.2-20 crmn_0.0.21 cli_3.6.3 ## [100] RcppParallel_5.1.8 rpart_4.1.23 systemfonts_1.2.3 ## [103] munsell_0.5.1 jquerylib_0.1.4 Rcpp_1.0.13 ## [106] globals_0.16.3 parallel_4.4.1 gower_1.0.1 ## [109] blob_1.2.4 bitops_1.0-7 listenv_0.9.1 ## [112] viridisLite_0.4.2 ipred_0.9-15 e1071_1.7-14 ## [115] scales_1.3.0 prodlim_2024.06.25 rlang_1.1.4 ## [118] fastmatch_1.1-4 "],["metaboanalyst-pipeline.html", "Section 3 Metaboanalyst Pipeline 3.1 Normality Assessment 3.2 MetaboAnalystR Object Initialization 3.3 Auto-scaleing (z-score Normalization) 3.4 Post-Normalization Normality Check 3.5 Fold change Analysis 3.6 Differential Expression Analysis", " Section 3 Metaboanalyst Pipeline Description: This pipeline performs a complete statistical workflow for preprocessing and differential analysis of metabolomics data using MetaboAnalystR package. Project Initialization mypath= &quot;C:/Users/USER/Documents/Github/CRC_project/&quot; dir.create(&quot;output&quot;) dir.create(&quot;plots&quot;) dir.create(&quot;input&quot;) #Load libraries library(tibble) library(plyr) library(dplyr) library(tidyverse) library(openxlsx) library(cowplot) library(ggplot2) library(&quot;MetaboAnalystR&quot;) #load data data= read.csv(paste0(mypath, &quot;input/data_for_downstream.csv&quot;)) data = data |&gt; column_to_rownames(colnames(data)[1]) 3.1 Normality Assessment A custom check_normality() function evaluates the distribution of metabolite intensities before and after normalization using: Shapiro-Wilk test on sample means &amp; Density and QQ plots for visual inspection. # check normality of the data check_normality &lt;- function(data, output_dir = paste0(mypath, &quot;plots/&quot;), prefix = &quot;before_norm&quot;) { if (!dir.exists(output_dir)) dir.create(output_dir, recursive = TRUE) # Convert to numeric data &lt;- mutate_all(data, ~ as.numeric(as.character(.))) # Row-wise means row_means &lt;- apply(data, 1, mean, na.rm = TRUE) # Shapiro-Wilk test shapiro_result &lt;- shapiro.test(row_means) message &lt;- if (shapiro_result$p.value &gt; 0.05) { &quot;Parametric&quot; } else { &quot;Non-parametric&quot; } # Density Plot df_means &lt;- data.frame(value = row_means ) dens_plot &lt;- ggplot(df_means, aes(x = value)) + geom_density(fill = &quot;#69b3a2&quot;, alpha = 0.8, color = &quot;black&quot;, size = 0.4) + geom_rug(alpha = 0.2) + labs(title = &quot;Density of Metabolites Means&quot;, x = &quot;Mean Intensity&quot;, y = &quot;Density&quot;) + theme_minimal(base_size = 14) + theme( plot.background = element_rect(fill = &quot;white&quot;, color = NA), panel.background = element_rect(fill = &quot;white&quot;, color = NA), panel.grid = element_blank(), axis.line = element_line(color = &quot;black&quot;), axis.ticks = element_line(color = &quot;black&quot;), text = element_text(face = &quot;bold&quot;), plot.title = element_text(hjust = 0.5) ) # QQ-Plot df_means &lt;- data.frame(value = row_means) qq_plot &lt;- ggplot(df_means, aes(sample = value)) + geom_qq(linewidth = 2.5, alpha = 0.7) + geom_qq_line(linewidth = 0.7, colour = &quot;red&quot;) + labs(title = &quot;QQ Plot of Metabolite Means&quot;, x = &quot;Theoretical&quot;, y = &quot;Intensity&quot;) + theme_minimal(base_size = 14) + theme( plot.background = element_rect(fill = &quot;white&quot;, color = NA), panel.background = element_rect(fill = &quot;white&quot;, color = NA), panel.grid = element_blank(), axis.line = element_line(color = &quot;black&quot;), axis.ticks = element_line(color = &quot;black&quot;), text = element_text(face = &quot;bold&quot;), plot.title = element_text(hjust = 0.5) ) # Combine both plots side by side combined &lt;- plot_grid(dens_plot, qq_plot, labels = c(&quot;A&quot;, &quot;B&quot;), label_size = 16) # Save combined plot ggsave(filename = paste0(output_dir, paste0(&quot;combined_&quot;, prefix, &quot;.png&quot;)), plot = combined, dpi = 600, width = 14, height = 6, bg = &quot;white&quot;) print(message) print(combined) return(list( normality = message, shapiro_p_value = shapiro_result$p.value )) } #Apply the function before_norm= check_normality(data, prefix = &quot;before_norm&quot;) ## [1] &quot;Non-parametric&quot; 3.2 MetaboAnalystR Object Initialization Missing value replacement, Data sanity checks, Automatic normalization Summary plots for metabolite and sample normalization # adjust the input format for metaboanalyst group_dist= gsub(&quot;_.*&quot;, &quot;&quot;, colnames(data)) print(group_dist) ## [1] &quot;CRC&quot; &quot;CRC&quot; &quot;CRC&quot; &quot;CRC&quot; &quot;CRC&quot; &quot;CRC&quot; &quot;CRC&quot; &quot;CRC&quot; &quot;CRC&quot; &quot;CRC&quot; ## [11] &quot;Ctrl&quot; &quot;Ctrl&quot; &quot;Ctrl&quot; &quot;Ctrl&quot; &quot;Ctrl&quot; &quot;Ctrl&quot; &quot;Ctrl&quot; &quot;Ctrl&quot; &quot;Ctrl&quot; &quot;Ctrl&quot; data_formatted= rbind(group_dist, data) write.csv(data_formatted, paste0(mypath,&quot;output/for_metaboanalyst.csv&quot;)) 3.3 Auto-scaleing (z-score Normalization) #&#39; ## setwd(&quot;New folder/&quot;) mSet&lt;-InitDataObjects(&quot;pktable&quot;, &quot;stat&quot;, FALSE) mSet&lt;-Read.TextData(mSet, paste0(mypath,&quot;output/for_metaboanalyst.csv&quot;), &quot;colu&quot;, &quot;disc&quot;) mSet&lt;-SanityCheckData(mSet) ## [1] &quot;Successfully passed sanity check!&quot; ## [2] &quot;Samples are not paired.&quot; ## [3] &quot;2 groups were detected in samples.&quot; ## [4] &quot;Only English letters, numbers, underscore, hyphen and forward slash (/) are allowed.&quot; ## [5] &quot;&lt;font color=\\&quot;orange\\&quot;&gt;Other special characters or punctuations (if any) will be stripped off.&lt;/font&gt;&quot; ## [6] &quot;All data values are numeric.&quot; ## [7] &quot;A total of 0 (0%) missing values were detected.&quot; ## [8] &quot;&lt;u&gt;By default, missing values will be replaced by 1/5 of min positive values of their corresponding variables&lt;/u&gt;&quot; ## [9] &quot;Click the &lt;b&gt;Proceed&lt;/b&gt; button if you accept the default practice;&quot; ## [10] &quot;Or click the &lt;b&gt;Missing Values&lt;/b&gt; button to use other methods.&quot; mSet&lt;-ReplaceMin(mSet) mSet&lt;-PreparePrenormData(mSet) mSet&lt;-Normalization(mSet, &quot;NULL&quot;, &quot;NULL&quot;, &quot;AutoNorm&quot;, ratio=FALSE) mSet&lt;-PlotNormSummary(mSet, paste0(mypath,&quot;plots/metabolites_norm&quot;), &quot;png&quot;, 600, width=NA) mSet&lt;-PlotSampleNormSummary(mSet, paste0(mypath,&quot;plots/sample_norm&quot;), &quot;png&quot;, 600, width=NA) 3.4 Post-Normalization Normality Check #normality check after normalization X &lt;- mSet$dataSet$norm after_norm= check_normality(X, prefix = &quot;after_norm&quot;) ## [1] &quot;Non-parametric&quot; 3.5 Fold change Analysis # Fold Change Analysis mSet&lt;-FC.Anal(mSet, 2, 0, FALSE) fc &lt;- mSet$analSet$fc fc_df = data.frame(FC = fc$fc.all, logFC = fc$fc.log) 3.6 Differential Expression Analysis #Differential expression if(after_norm$normality== &quot;Non-parametric&quot;){ mSet&lt;-Ttests.Anal(mSet, nonpar = T, 0.05, FALSE, TRUE, &quot;fdr&quot;, all_results = TRUE) }else{ mSet&lt;-Ttests.Anal(mSet, nonpar = F, 0.05, FALSE, TRUE, &quot;fdr&quot;, all_results = TRUE) } ## [1] &quot;Performing regular t-tests ....&quot; ## [1] &quot;A total of 134 significant features were found.&quot; tt.sig= mSet$analSet$tt$sig.mat |&gt; as.data.frame() fc.sig= fc_df[ match( row.names(tt.sig), row.names(fc_df)) , ] sig_df= cbind(tt.sig, fc.sig ) tt.all= data.frame( abs.t.score= mSet[[&quot;analSet&quot;]][[&quot;tt&quot;]][[&quot;t.score&quot;]], p.value= mSet[[&quot;analSet&quot;]][[&quot;tt&quot;]][[&quot;p.value&quot;]], FDR = p.adjust(mSet[[&quot;analSet&quot;]][[&quot;tt&quot;]][[&quot;p.value&quot;]], method = &quot;BH&quot;) # Benjamini-Hochberg ) fc.all= fc_df[ match( row.names(tt.all), row.names(fc_df)) , ] all_df= cbind(tt.all, fc.all ) print (&quot;DE is Done!&quot;) ## [1] &quot;DE is Done!&quot; Export Results #Export write.csv(sig_df , paste0(mypath, &quot;output/DE_sig.csv&quot;) , row.names = TRUE) write.csv(all_df , paste0(mypath,&quot;output/DE_all.csv&quot;) , row.names = TRUE) ## R version 4.4.1 (2024-06-14 ucrt) ## Platform: x86_64-w64-mingw32/x64 ## Running under: Windows 10 x64 (build 19045) ## ## Matrix products: default ## ## ## locale: ## [1] LC_COLLATE=English_United States.utf8 ## [2] LC_CTYPE=English_United States.utf8 ## [3] LC_MONETARY=English_United States.utf8 ## [4] LC_NUMERIC=C ## [5] LC_TIME=English_United States.utf8 ## ## time zone: Africa/Cairo ## tzcode source: internal ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] memoise_2.0.1 caret_6.0-94 lattice_0.22-6 ## [4] pls_2.8-3 Rserve_1.8-13 MetaboAnalystR_3.2.0 ## [7] cowplot_1.1.3 DT_0.33 openxlsx_4.2.6.1 ## [10] lubridate_1.9.3 forcats_1.0.0 stringr_1.5.1 ## [13] purrr_1.0.2 readr_2.1.5 tidyr_1.3.1 ## [16] ggplot2_3.5.1 tidyverse_2.0.0 dplyr_1.1.4 ## [19] plyr_1.8.9 tibble_3.2.1 ## ## loaded via a namespace (and not attached): ## [1] RColorBrewer_1.1-3 rstudioapi_0.16.0 jsonlite_1.8.8 ## [4] magrittr_2.0.3 farver_2.1.2 rmarkdown_2.27 ## [7] ragg_1.3.2 vctrs_0.6.5 multtest_2.61.0 ## [10] Cairo_1.6-2 htmltools_0.5.8.1 pROC_1.18.5 ## [13] sass_0.4.9 parallelly_1.38.0 KernSmooth_2.23-24 ## [16] bslib_0.8.0 htmlwidgets_1.6.4 impute_1.79.0 ## [19] plotly_4.10.4 cachem_1.1.0 igraph_2.0.3 ## [22] lifecycle_1.0.4 iterators_1.0.14 pkgconfig_2.0.3 ## [25] Matrix_1.7-0 R6_2.5.1 fastmap_1.2.0 ## [28] future_1.33.2 digest_0.6.36 pcaMethods_1.97.0 ## [31] colorspace_2.1-1 siggenes_1.79.0 textshaping_0.4.0 ## [34] crosstalk_1.2.1 ellipse_0.5.0 RSQLite_2.3.7 ## [37] labeling_0.4.3 timechange_0.3.0 httr_1.4.7 ## [40] compiler_4.4.1 proxy_0.4-27 bit64_4.0.5 ## [43] withr_3.0.0 glasso_1.11 BiocParallel_1.39.0 ## [46] DBI_1.2.3 qs_0.26.3 highr_0.11 ## [49] gplots_3.1.3.1 MASS_7.3-60.2 lava_1.8.0 ## [52] gtools_3.9.5 caTools_1.18.2 ModelMetrics_1.2.2.2 ## [55] tools_4.4.1 zip_2.3.1 future.apply_1.11.2 ## [58] nnet_7.3-19 glue_1.7.0 nlme_3.1-164 ## [61] grid_4.4.1 reshape2_1.4.4 fgsea_1.31.0 ## [64] generics_0.1.3 recipes_1.1.0 gtable_0.3.5 ## [67] tzdb_0.4.0 class_7.3-22 data.table_1.15.4 ## [70] RApiSerialize_0.1.3 hms_1.1.3 stringfish_0.16.0 ## [73] BiocGenerics_0.52.0 foreach_1.5.2 pillar_1.11.0 ## [76] limma_3.61.5 splines_4.4.1 survival_3.6-4 ## [79] bit_4.0.5 tidyselect_1.2.1 locfit_1.5-9.10 ## [82] knitr_1.48 bookdown_0.40 edgeR_4.3.5 ## [85] stats4_4.4.1 xfun_0.46 Biobase_2.64.0 ## [88] scrime_1.3.5 statmod_1.5.0 hardhat_1.4.0 ## [91] timeDate_4032.109 stringi_1.8.4 lazyeval_0.2.2 ## [94] yaml_2.3.10 evaluate_0.24.0 codetools_0.2-20 ## [97] crmn_0.0.21 cli_3.6.3 RcppParallel_5.1.8 ## [100] rpart_4.1.23 systemfonts_1.2.3 munsell_0.5.1 ## [103] jquerylib_0.1.4 Rcpp_1.0.13 globals_0.16.3 ## [106] parallel_4.4.1 gower_1.0.1 blob_1.2.4 ## [109] bitops_1.0-7 listenv_0.9.1 viridisLite_0.4.2 ## [112] ipred_0.9-15 e1071_1.7-14 scales_1.3.0 ## [115] prodlim_2024.06.25 rlang_1.1.4 fastmatch_1.1-4 "],["downstream-visualizations.html", "Section 4 Downstream Visualizations 4.1 Volcano Plot 4.2 Heatmap Plot 4.3 Box Plots", " Section 4 Downstream Visualizations Description: This pipeline performs downstream visualizations, including volcano plot, heatmap, and box plots. Project Initialization: #Sets the working directory and creates subfolders for organizing outputs. mypath= &quot;C:/Users/USER/Documents/Github/CRC_project/&quot; dir.create(&quot;output&quot;) dir.create(&quot;plots&quot;) dir.create(&quot;input&quot;) #Load libraries library(tibble) library(plyr) library(dplyr) library(tidyverse) library(openxlsx) library(cowplot) library(ggplot2) library(tibble) library(&quot;RColorBrewer&quot;) library(&quot;circlize&quot;) library(ComplexHeatmap) library(biomaRt) library(dplyr) library(plyr) library(fields) library(tidyr) library(RColorBrewer) library(viridis) library(ggplot2) library(ggrepel) #load data data= read.csv(paste0(mypath,&quot;input/data_for_downstream.csv&quot;)) data = data %&gt;% column_to_rownames(colnames(data)[1]) %&gt;% dplyr::select(-1) de= read.csv(paste0(mypath,&quot;output/DE_sig.csv&quot;)) de=de |&gt; filter(abs(logFC) &gt;= log2(3)) output_dir= paste0(mypath, &quot;output/&quot;) # set group distribution for samples group_dist= gsub(&quot;_.*&quot;, &quot;&quot;, colnames(data)) print(group_dist) ## [1] &quot;CRC&quot; &quot;CRC&quot; &quot;CRC&quot; &quot;CRC&quot; &quot;CRC&quot; &quot;CRC&quot; &quot;CRC&quot; &quot;CRC&quot; &quot;CRC&quot; &quot;Ctrl&quot; ## [11] &quot;Ctrl&quot; &quot;Ctrl&quot; &quot;Ctrl&quot; &quot;Ctrl&quot; &quot;Ctrl&quot; &quot;Ctrl&quot; &quot;Ctrl&quot; &quot;Ctrl&quot; &quot;Ctrl&quot; group_levels= unique(group_dist) group_colors &lt;- c(&quot;#fc8d62&quot;, &quot;#66c2a5&quot;) names(group_colors) &lt;- group_levels 4.1 Volcano Plot plot_volcano = function(file_path=NULL, df=NULL, name=NULL, plot_dir = &quot;plots&quot;, fc_threshold = 1.5) { if (!dir.exists(plot_dir)) dir.create(plot_dir, recursive = TRUE) # Read the DE result if(!is.null(file_path)){ res &lt;- read.csv(file_path) name &lt;- gsub(&quot;.*_([[:alnum:] ]+)\\\\.csv$&quot;, &quot;\\\\1&quot;, basename(file_path)) } if(!is.null(df) &amp; !is.null(name)){ res= df name= name } res &lt;- as.data.frame(res) colnames(res)[grepl(&quot;log&quot;,colnames(res) )]= &quot;logFC&quot; colnames(res)[grepl(&quot;adj|FDR&quot;,colnames(res) )]= &quot;padj&quot; res$padj[is.na(res$padj)]= res$pval[is.na(res$padj)] # Define direction res$Direction &lt;- ifelse(res$padj &lt;= 0.05 &amp; res$logFC &gt;= log2(fc_threshold), &quot;Up&quot;, ifelse(res$padj &lt;= 0.05 &amp; res$logFC &lt;= -log2(fc_threshold), &quot;Down&quot;, &quot;Non-significant&quot;)) # Keep only labels for up or down expressed features res &lt;- res %&gt;% mutate(features = ifelse(Direction %in% c(&quot;Up&quot;, &quot;Down&quot;), X, &quot;&quot;)) # Compute -log10(p-value) res$log10p &lt;- -log10(res$padj) # Plot parameters xminma &lt;- -3.5 xmaxma &lt;- 3.5 # Generate the plot volcano &lt;- ggplot(res, aes(x = logFC, y = log10p, color = Direction, label = features)) + geom_point(size = 1.2, alpha = 0.7) + geom_rug(alpha = 0.6) + scale_color_manual(values = c(&quot;Up&quot; = &quot;red2&quot;, &quot;Down&quot; = &quot;darkslateblue&quot;, &quot;Non-significant&quot; = &quot;grey66&quot;)) + xlab(&#39;log2 Fold Change&#39;) + ylab(&#39;-log10 adj.p-value&#39;) + #scale_x_continuous(limits = c(xminma, xmaxma)) + theme_bw() + theme(legend.title = element_blank()) + geom_vline(xintercept = c(-log2(fc_threshold), 0, log2(fc_threshold)), linetype = c(&quot;dotted&quot;, &quot;solid&quot;, &quot;dotted&quot;)) + geom_hline(yintercept = -log10(0.05), linetype = &quot;dashed&quot;) + geom_text_repel(aes(label = features), size = 2, max.overlaps = 10, segment.color = &quot;grey50&quot;, color = &quot;black&quot;) + ggtitle(paste0(&quot;Volcano Plot of DE &quot;, name)) # Print and save print(volcano) ggsave(filename = paste0(plot_dir, &quot;/Volcanoplot_&quot;, name, &quot;.jpg&quot;), plot = volcano, dpi = 600, width = 7, height = 4) } #Apply the function plot_volcano(file_path= paste0(mypath,&quot;output/DE_all.csv&quot;)) 4.2 Heatmap Plot plot_heatmap= function(data= NULL, de= NULL, group_dist, group_colors ){ #you can use for loop for multiple groups DE files data_sub= data[row.names(data) %in% de$X, ] #data= data %&gt;% dplyr::slice(1:50) #row.names= gsub(&quot;\\\\%.*&quot;, &quot;&quot;, row.names(data)) heat_data= t(scale(t(data_sub))) # center row wise rownames(heat_data)= substr(rownames(heat_data), 1, 25) ta &lt;- HeatmapAnnotation( Condition = group_dist, col = list( Condition = group_colors ), annotation_height = unit(10, &quot;mm&quot;) ) #palt= colorRampPalette(c(&quot;blue4&quot;, &quot;black&quot;, &quot;#8B0000&quot;))(256) #palt= colorRampPalette(viridis(n = 8))(200) palt= colorRamp2(c(-3, 0, 3), c(&quot;#053061&quot;, &quot;#F7F5F4&quot;, &quot;darkred&quot;)) heatmap &lt;- Heatmap( matrix = as.matrix(heat_data), name = &quot;Normalized Intensity&quot;, col =palt ,#&#39; ## &#39; #&#39; ## #&#39; ## matlab::jet.colors(200), show_row_names = TRUE, cluster_rows = TRUE, cluster_columns = TRUE, show_column_names = FALSE, row_names_gp = gpar(fontsize = 8), top_annotation = ta, heatmap_legend_param = list( title= &quot;Scaled\\nIntensity&quot;, title_gp = gpar(fontsize = 10, fontface = &quot;bold&quot;), labels_gp = gpar(fontsize = 10), legend_height = unit(4, &quot;cm&quot;), legend_width = unit(.5, &quot;cm&quot;) ) ) print(heatmap) png(&quot;plots/heatmap.png&quot;,width = 4000, height = 5000, res = 600) draw(heatmap, annotation_legend_side = &quot;right&quot;, heatmap_legend_side = &quot;right&quot;) #&quot;right&quot; dev.off() } #Apply the function plot_heatmap(data, de, group_dist, group_colors) ## png ## 2 4.3 Box Plots # Loop over DE features plot_box= function( data=NULL, de= NULL, group_dist){ data= data[row.names(data) %in% de$X, ] data.t= t(data) de &lt;- as.data.frame(de) colnames(de)[grepl(&quot;log&quot;,colnames(de) )]= &quot;logFC&quot; colnames(de)[grepl(&quot;adj|FDR&quot;,colnames(de) )]= &quot;padj&quot; de$padj[is.na(de$padj)]= de$pval[is.na(de$padj)] for (m in colnames(data.t)) { # Subset data data.box = data.frame( value= data.t[, m], group= group_dist) FDR= de$padj[de$X== m] quartile.t= quantile( data.box$value, 0.90) p= ggplot(data.box , aes(x = group , y = value , fill= group)) + geom_boxplot() + scale_fill_manual(values = group_colors )+ labs(title = paste0(&quot;Box Plot of Intensity for &quot;, m, &quot; Metabolite&quot;), x = &quot;&quot;, y = &quot;Intensity&quot;, fill= &quot;Groups&quot;)+ annotate(&quot;text&quot;, x = 3.5, y = quartile.t, label = paste0( &quot;FDR= &quot; , round(FDR, 2)) , size = 3, fontface = &quot;italic&quot;, hjust = 1) + theme_minimal()+ theme( axis.text.x = element_text(angle = 0, hjust = .65, size = 7), axis.text.y = element_text(size = 7), plot.title = element_text(size = 9), axis.line = element_line(color = &quot;black&quot;), panel.grid = element_blank(), axis.ticks = element_blank(), panel.border = element_blank()) print(p) ggsave(paste0(&quot;plots/boxplot_&quot; , m, &quot;.png&quot;), p, width=5, height =4, dpi=600) } } #Apply the function plot_box(data, de[1:3,], group_dist ) ## R version 4.4.1 (2024-06-14 ucrt) ## Platform: x86_64-w64-mingw32/x64 ## Running under: Windows 10 x64 (build 19045) ## ## Matrix products: default ## ## ## locale: ## [1] LC_COLLATE=English_United States.utf8 ## [2] LC_CTYPE=English_United States.utf8 ## [3] LC_MONETARY=English_United States.utf8 ## [4] LC_NUMERIC=C ## [5] LC_TIME=English_United States.utf8 ## ## time zone: Africa/Cairo ## tzcode source: internal ## ## attached base packages: ## [1] grid stats graphics grDevices utils datasets methods ## [8] base ## ## other attached packages: ## [1] ggrepel_0.9.6 viridis_0.6.5 fields_16.2 ## [4] viridisLite_0.4.2 spam_2.10-0 biomaRt_2.61.2 ## [7] ComplexHeatmap_2.21.0 circlize_0.4.16 RColorBrewer_1.1-3 ## [10] memoise_2.0.1 caret_6.0-94 lattice_0.22-6 ## [13] pls_2.8-3 Rserve_1.8-13 MetaboAnalystR_3.2.0 ## [16] cowplot_1.1.3 DT_0.33 openxlsx_4.2.6.1 ## [19] lubridate_1.9.3 forcats_1.0.0 stringr_1.5.1 ## [22] purrr_1.0.2 readr_2.1.5 tidyr_1.3.1 ## [25] ggplot2_3.5.1 tidyverse_2.0.0 dplyr_1.1.4 ## [28] plyr_1.8.9 tibble_3.2.1 ## ## loaded via a namespace (and not attached): ## [1] splines_4.4.1 filelock_1.0.3 bitops_1.0-7 ## [4] hardhat_1.4.0 pROC_1.18.5 rpart_4.1.23 ## [7] httr2_1.0.2 lifecycle_1.0.4 edgeR_4.3.5 ## [10] doParallel_1.0.17 globals_0.16.3 MASS_7.3-60.2 ## [13] scrime_1.3.5 crosstalk_1.2.1 magrittr_2.0.3 ## [16] limma_3.61.5 plotly_4.10.4 sass_0.4.9 ## [19] rmarkdown_2.27 jquerylib_0.1.4 yaml_2.3.10 ## [22] zip_2.3.1 DBI_1.2.3 maps_3.4.2 ## [25] zlibbioc_1.50.0 BiocGenerics_0.52.0 nnet_7.3-19 ## [28] rappdirs_0.3.3 ipred_0.9-15 GenomeInfoDbData_1.2.12 ## [31] lava_1.8.0 IRanges_2.38.1 S4Vectors_0.42.1 ## [34] listenv_0.9.1 ellipse_0.5.0 parallelly_1.38.0 ## [37] codetools_0.2-20 xml2_1.3.6 RApiSerialize_0.1.3 ## [40] tidyselect_1.2.1 shape_1.4.6.1 UCSC.utils_1.1.0 ## [43] farver_2.1.2 BiocFileCache_2.13.0 matrixStats_1.3.0 ## [46] stats4_4.4.1 jsonlite_1.8.8 GetoptLong_1.0.5 ## [49] multtest_2.61.0 e1071_1.7-14 survival_3.6-4 ## [52] iterators_1.0.14 systemfonts_1.2.3 foreach_1.5.2 ## [55] progress_1.2.3 tools_4.4.1 ragg_1.3.2 ## [58] Rcpp_1.0.13 glue_1.7.0 gridExtra_2.3 ## [61] prodlim_2024.06.25 xfun_0.46 GenomeInfoDb_1.41.1 ## [64] crmn_0.0.21 withr_3.0.0 fastmap_1.2.0 ## [67] caTools_1.18.2 digest_0.6.36 timechange_0.3.0 ## [70] R6_2.5.1 textshaping_0.4.0 colorspace_2.1-1 ## [73] Cairo_1.6-2 gtools_3.9.5 RSQLite_2.3.7 ## [76] generics_0.1.3 data.table_1.15.4 recipes_1.1.0 ## [79] class_7.3-22 prettyunits_1.2.0 httr_1.4.7 ## [82] htmlwidgets_1.6.4 ModelMetrics_1.2.2.2 pkgconfig_2.0.3 ## [85] gtable_0.3.5 timeDate_4032.109 blob_1.2.4 ## [88] siggenes_1.79.0 impute_1.79.0 XVector_0.44.0 ## [91] htmltools_0.5.8.1 dotCall64_1.1-1 bookdown_0.40 ## [94] fgsea_1.31.0 clue_0.3-65 scales_1.3.0 ## [97] Biobase_2.64.0 png_0.1-8 gower_1.0.1 ## [100] knitr_1.48 rstudioapi_0.16.0 tzdb_0.4.0 ## [103] reshape2_1.4.4 rjson_0.2.21 curl_5.2.1 ## [106] nlme_3.1-164 proxy_0.4-27 cachem_1.1.0 ## [109] GlobalOptions_0.1.2 KernSmooth_2.23-24 parallel_4.4.1 ## [112] AnnotationDbi_1.67.0 pillar_1.11.0 vctrs_0.6.5 ## [115] gplots_3.1.3.1 pcaMethods_1.97.0 stringfish_0.16.0 ## [118] dbplyr_2.5.0 cluster_2.1.6 evaluate_0.24.0 ## [121] magick_2.8.4 cli_3.6.3 locfit_1.5-9.10 ## [124] compiler_4.4.1 rlang_1.1.4 crayon_1.5.3 ## [127] future.apply_1.11.2 labeling_0.4.3 stringi_1.8.4 ## [130] BiocParallel_1.39.0 munsell_0.5.1 Biostrings_2.72.1 ## [133] lazyeval_0.2.2 Matrix_1.7-0 hms_1.1.3 ## [136] glasso_1.11 bit64_4.0.5 future_1.33.2 ## [139] KEGGREST_1.45.1 statmod_1.5.0 highr_0.11 ## [142] qs_0.26.3 igraph_2.0.3 RcppParallel_5.1.8 ## [145] bslib_0.8.0 fastmatch_1.1-4 bit_4.0.5 "],["functional-enrichment.html", "Section 5 Functional Enrichment 5.1 Data Preparation 5.2 Enrichment plot", " Section 5 Functional Enrichment Description: This pipeline generates dot plot visualizations for the enriched pathways. The results can be obtained from applying either Metabolite Set Enrichment Analysis (MSEA) or Over-Representation Analysis (ORA) performed on the MetaboAnalyst online platform. Project Initialization: #Sets the working directory and creates subfolders for organizing outputs. mypath= &quot;C:/Users/USER/Documents/Github/CRC_project/&quot; dir.create(&quot;output&quot;) dir.create(&quot;plots&quot;) dir.create(&quot;input&quot;) #Load libraries library(tibble) library(plyr) library(dplyr) library(tidyverse) library(openxlsx) library(cowplot) library(ggplot2) # The enrichment was applied using Metaboanalyst online website # load enrichment results enrich_table = read.csv(paste0(mypath,&quot;input/enrichment_table.csv&quot;)) colnames(enrich_table) ## [1] &quot;new_pathway_name&quot; &quot;Total.Cmpd&quot; &quot;Hits&quot; &quot;Statistic.Q&quot; ## [5] &quot;Expected.Q&quot; &quot;Raw.p&quot; &quot;Holm.p&quot; &quot;FDR&quot; ## [9] &quot;fold_enrichment&quot; &quot;P.adj.FDR.log&quot; &quot;signif&quot; 5.1 Data Preparation # add necessary columns for plotting #enrich_table$fold_enrichment &lt;- enrich_table$hits /enrich_table$total enrich_table$fold_enrichment &lt;- enrich_table$Hits /enrich_table$Total.Cmpd #enrich_table$fold_enrichment &lt;- round(enrich_table$fold_enrichment,2) enrich_table$FDR.log &lt;- -log10(enrich_table$FDR) enrich_table$signif &lt;- ifelse(enrich_table$P.adj.FDR.log &gt;= -log10(.05), &quot;sig&quot;, &quot;non&quot;) table( enrich_table$signif) ## ## sig ## 39 # adjust pathway names colnames(enrich_table)[1] &lt;- &quot;new_pathway_name&quot; enrich_table$new_pathway_name= gsub(&quot;%.*&quot;, &quot;&quot;, enrich_table$new_pathway_name) # remove duplicated pathways enrich_table &lt;- enrich_table[!duplicated(enrich_table$new_pathway_name),] # plot only significant pathways enrich_table &lt;- enrich_table[enrich_table$signif== &quot;sig&quot;,] 5.2 Enrichment plot plot1 &lt;- ggplot(data=enrich_table, aes( y = reorder(new_pathway_name, FDR.log), x = FDR.log, size = fold_enrichment, color = FDR )) + geom_point() + scale_size_continuous(guide = guide_legend(order = 2), breaks = c( 0.03, 0.07, 0.1, 0.2, 0.3) ) + scale_color_gradient(low = &quot;red&quot;, high = &quot;yellow&quot;, name = &quot;FDR&quot;) + #breaks = c(min(enrich_table$FDR), 0.25,0.50, 0.75, max(enrich_table$FDR)), #labels = c( round(min(enrich_table$FDR),2), 0.25, &quot;0.50&quot;, 0.75, round(max(enrich_table$FDR),2))) + # Customize the color scale labs(size = &quot;Hits/Total&quot;) + xlab(&quot;-log10(FDR)&quot;) + ylab(&quot;Pathway&quot;) + theme_bw() + theme(legend.position=&quot;right&quot;, text = element_text(face=&quot;bold&quot;), axis.text = element_text(color = &quot;black&quot;, face = &quot;bold&quot;)) + theme(axis.text = element_text(color = &quot;black&quot;, face = &quot;bold&quot;, size = 10)) print(plot1) ggsave(paste0(mypath,&quot;plots/enrichment_plot.jpg&quot;), plot1, dpi = 600, width = 7, height = 7) ## R version 4.4.1 (2024-06-14 ucrt) ## Platform: x86_64-w64-mingw32/x64 ## Running under: Windows 10 x64 (build 19045) ## ## Matrix products: default ## ## ## locale: ## [1] LC_COLLATE=English_United States.utf8 ## [2] LC_CTYPE=English_United States.utf8 ## [3] LC_MONETARY=English_United States.utf8 ## [4] LC_NUMERIC=C ## [5] LC_TIME=English_United States.utf8 ## ## time zone: Africa/Cairo ## tzcode source: internal ## ## attached base packages: ## [1] grid stats graphics grDevices utils datasets methods ## [8] base ## ## other attached packages: ## [1] ggrepel_0.9.6 viridis_0.6.5 fields_16.2 ## [4] viridisLite_0.4.2 spam_2.10-0 biomaRt_2.61.2 ## [7] ComplexHeatmap_2.21.0 circlize_0.4.16 RColorBrewer_1.1-3 ## [10] memoise_2.0.1 caret_6.0-94 lattice_0.22-6 ## [13] pls_2.8-3 Rserve_1.8-13 MetaboAnalystR_3.2.0 ## [16] cowplot_1.1.3 DT_0.33 openxlsx_4.2.6.1 ## [19] lubridate_1.9.3 forcats_1.0.0 stringr_1.5.1 ## [22] purrr_1.0.2 readr_2.1.5 tidyr_1.3.1 ## [25] ggplot2_3.5.1 tidyverse_2.0.0 dplyr_1.1.4 ## [28] plyr_1.8.9 tibble_3.2.1 ## ## loaded via a namespace (and not attached): ## [1] splines_4.4.1 filelock_1.0.3 bitops_1.0-7 ## [4] hardhat_1.4.0 pROC_1.18.5 rpart_4.1.23 ## [7] httr2_1.0.2 lifecycle_1.0.4 edgeR_4.3.5 ## [10] doParallel_1.0.17 globals_0.16.3 MASS_7.3-60.2 ## [13] scrime_1.3.5 crosstalk_1.2.1 magrittr_2.0.3 ## [16] limma_3.61.5 plotly_4.10.4 sass_0.4.9 ## [19] rmarkdown_2.27 jquerylib_0.1.4 yaml_2.3.10 ## [22] zip_2.3.1 DBI_1.2.3 maps_3.4.2 ## [25] zlibbioc_1.50.0 BiocGenerics_0.52.0 nnet_7.3-19 ## [28] rappdirs_0.3.3 ipred_0.9-15 GenomeInfoDbData_1.2.12 ## [31] lava_1.8.0 IRanges_2.38.1 S4Vectors_0.42.1 ## [34] listenv_0.9.1 ellipse_0.5.0 parallelly_1.38.0 ## [37] codetools_0.2-20 xml2_1.3.6 RApiSerialize_0.1.3 ## [40] tidyselect_1.2.1 shape_1.4.6.1 UCSC.utils_1.1.0 ## [43] farver_2.1.2 BiocFileCache_2.13.0 matrixStats_1.3.0 ## [46] stats4_4.4.1 jsonlite_1.8.8 GetoptLong_1.0.5 ## [49] multtest_2.61.0 e1071_1.7-14 survival_3.6-4 ## [52] iterators_1.0.14 systemfonts_1.2.3 foreach_1.5.2 ## [55] progress_1.2.3 tools_4.4.1 ragg_1.3.2 ## [58] Rcpp_1.0.13 glue_1.7.0 gridExtra_2.3 ## [61] prodlim_2024.06.25 xfun_0.46 GenomeInfoDb_1.41.1 ## [64] crmn_0.0.21 withr_3.0.0 fastmap_1.2.0 ## [67] caTools_1.18.2 digest_0.6.36 timechange_0.3.0 ## [70] R6_2.5.1 textshaping_0.4.0 colorspace_2.1-1 ## [73] Cairo_1.6-2 gtools_3.9.5 RSQLite_2.3.7 ## [76] generics_0.1.3 data.table_1.15.4 recipes_1.1.0 ## [79] class_7.3-22 prettyunits_1.2.0 httr_1.4.7 ## [82] htmlwidgets_1.6.4 ModelMetrics_1.2.2.2 pkgconfig_2.0.3 ## [85] gtable_0.3.5 timeDate_4032.109 blob_1.2.4 ## [88] siggenes_1.79.0 impute_1.79.0 XVector_0.44.0 ## [91] htmltools_0.5.8.1 dotCall64_1.1-1 bookdown_0.40 ## [94] fgsea_1.31.0 clue_0.3-65 scales_1.3.0 ## [97] Biobase_2.64.0 png_0.1-8 gower_1.0.1 ## [100] knitr_1.48 rstudioapi_0.16.0 tzdb_0.4.0 ## [103] reshape2_1.4.4 rjson_0.2.21 curl_5.2.1 ## [106] nlme_3.1-164 proxy_0.4-27 cachem_1.1.0 ## [109] GlobalOptions_0.1.2 KernSmooth_2.23-24 parallel_4.4.1 ## [112] AnnotationDbi_1.67.0 pillar_1.11.0 vctrs_0.6.5 ## [115] gplots_3.1.3.1 pcaMethods_1.97.0 stringfish_0.16.0 ## [118] dbplyr_2.5.0 cluster_2.1.6 evaluate_0.24.0 ## [121] magick_2.8.4 cli_3.6.3 locfit_1.5-9.10 ## [124] compiler_4.4.1 rlang_1.1.4 crayon_1.5.3 ## [127] future.apply_1.11.2 labeling_0.4.3 stringi_1.8.4 ## [130] BiocParallel_1.39.0 munsell_0.5.1 Biostrings_2.72.1 ## [133] lazyeval_0.2.2 Matrix_1.7-0 hms_1.1.3 ## [136] glasso_1.11 bit64_4.0.5 future_1.33.2 ## [139] KEGGREST_1.45.1 statmod_1.5.0 highr_0.11 ## [142] qs_0.26.3 igraph_2.0.3 RcppParallel_5.1.8 ## [145] bslib_0.8.0 fastmatch_1.1-4 bit_4.0.5 "],["wgcna-pipeline.html", "Section 6 WGCNA Pipeline 6.1 WGCNA Pipeline", " Section 6 WGCNA Pipeline Description: This pipeline automate WGCNA pipeline. So What is WGCNA? Weighted Gene/Metabolite Co-expression Network Analysis (WGCNA/WMCNA) WGCNA (for genes) and WMCNA (for metabolites) are network-based methods used to identify groups of co-expressed molecules, called modules, that may share common biological functions. The analysis starts by computing pairwise correlations (typically Pearson) between all features across samples. These correlations are then transformed into a weighted network using a soft-thresholding power, this help remove weak association- which ensures that the resulting network follows a scale-free topology, a common feature of biological systems. From the adjacency matrix, a topological overlap matrix (TOM) is calculated to measure how connected each pair of features is, considering both direct and shared connections. This leads to the identification of modules through hierarchical clustering and dynamic tree cutting, grouping together molecules with similar expression patterns. To relate modules to biological phenotypes, the module eigengenes (the first principal component of each module) are correlated with external traits, typically using Pearson correlation. When the phenotype is binary, it is encoded as 0 and 1. Modules that are significantly correlated with the phenotype of interest are then subjected to functional enrichment analysis to uncover associated pathways and biological functions. Finally, within these relevant modules, hub genes or metabolites—those with high intra-modular connectivity (i.e., high degree centrality)—are identified. These hubs are considered potential biomarker candidates or key regulatory features associated with the condition under study. Output Files: module_eigengenes.csv: Eigengenes (first principal component) for each module; used for trait correlation. gene_module_assignment.csv: Module assignment of each gene/metabolite (e.g., blue, brown, etc.). modules.csv: Lists of features grouped by module. Each column represents a module. module_membership.csv: Correlation (kME) of each gene with its module eigengene (i.e., intramodular connectivity). gene.trait.corr.csv: Correlation between each gene and the phenotype(s). module_hubs.csv: Hub genes/metabolites within each module (high kME &gt; hub_threshold). CytoscapeInput-edges-.txt: Edge list for each module’s network (TOM similarity &gt; 0.02); ready for Cytoscape. Project Initialization #Sets the working directory and creates subfolders for organizing outputs. mypath= &quot;C:/Users/USER/Documents/Github/CRC_project/&quot; dir.create(&quot;output&quot;) dir.create(&quot;plots&quot;) dir.create(&quot;input&quot;) #Load libraries library(tibble) library(plyr) library(dplyr) library(tidyverse) library(WGCNA) allowWGCNAThreads() ## Allowing multi-threading with up to 4 threads. #load data #expression data data= read.csv(paste0(mypath,&quot;input/data_for_downstream.csv&quot;)) data = data |&gt; column_to_rownames(colnames(data)[1]) group_dist= gsub(&quot;_.*&quot;, &quot;&quot;, colnames(data)) group_levels= unique(group_dist) #metadata metadata= data.frame(sample= colnames(data) , condition= group_dist) #metadata= read.csv(&quot;input/metadata_test.csv&quot; ) metadata = metadata |&gt; column_to_rownames(&quot;sample&quot;) metadata$condition= factor(metadata$condition, levels = group_levels) design= model.matrix(~ 0+condition , metadata) colnames(design)= gsub(&quot;condition&quot;,&quot;&quot;, colnames(design)) head(design) ## CRC Ctrl ## CRC_01 1 0 ## CRC_02 1 0 ## CRC_03 1 0 ## CRC_04 1 0 ## CRC_05 1 0 ## CRC_06 1 0 6.1 WGCNA Pipeline 6.1.1 Choose appropriate power for soft-thresholding # Elbow plot to choose the appropriate soft-thresholding power that ensures a scale-free (power-law) network topology. plotelbow &lt;- function(data) { # Choose a set of soft-thresholding powers (up to 20 is more informative) powers &lt;- c(1:10) # Pick the soft threshold based on scale-free topology criterion sft &lt;- pickSoftThreshold(t(data), powerVector = powers, verbose = 5) # Two plots side by side par(mfrow = c(1, 2)) # Plot the Scale-Free Topology Fit Index (R^2) plot(sft$fitIndices[, 1], -sign(sft$fitIndices[, 3]) * sft$fitIndices[, 2], xlab = &quot;Soft Threshold (power)&quot;, ylab = &quot;Scale Free Topology Model Fit, signed R^2&quot;, type = &quot;n&quot;, main = &quot;Scale Independence&quot;) text(sft$fitIndices[, 1], -sign(sft$fitIndices[, 3]) * sft$fitIndices[, 2], labels = powers, col = &quot;red&quot;) abline(h = 0.80, col = &quot;red&quot;, lty = 2) # R^2 threshold line # Plot the Mean Connectivity plot(sft$fitIndices[, 1], sft$fitIndices[, 5], xlab = &quot;Soft Threshold (power)&quot;, ylab = &quot;Mean Connectivity&quot;, type = &quot;n&quot;, main = &quot;Mean Connectivity&quot;) text(sft$fitIndices[, 1], sft$fitIndices[, 5], labels = powers, col = &quot;red&quot;) } #The chosen power should correspond to Signed R² (scale-free topology fit index) ≥ 0.9 &amp; #Mean connectivity is not too low (network should remain connected) # Display the plot plotelbow(data) ## pickSoftThreshold: will use block size 263. ## pickSoftThreshold: calculating connectivity for given powers... ## ..working on genes 1 through 263 of 263 ## Power SFT.R.sq slope truncated.R.sq mean.k. median.k. max.k. ## 1 1 0.0133 -0.203 -0.25200 83.8 76.00 133.0 ## 2 2 0.1260 -3.130 -0.12400 43.9 31.70 94.1 ## 3 3 0.1240 -2.570 -0.10400 30.1 16.60 78.5 ## 4 4 0.1920 -3.680 -0.03720 24.1 11.00 71.2 ## 5 5 0.2060 -3.590 -0.01890 21.0 7.74 67.5 ## 6 6 0.2390 -4.180 0.02530 19.2 6.00 65.3 ## 7 7 0.1910 -3.630 -0.02780 18.1 4.61 64.0 ## 8 8 0.2550 -4.350 0.05880 17.3 3.68 63.2 ## 9 9 0.1990 -3.710 -0.01210 16.8 3.06 62.6 ## 10 10 0.2090 -3.740 -0.00272 16.4 2.40 62.2 # save png(&quot;plots/elbow_plot.png&quot;, width = 1700, height = 1000, res = 300) plotelbow(data) ## pickSoftThreshold: will use block size 263. ## pickSoftThreshold: calculating connectivity for given powers... ## ..working on genes 1 through 263 of 263 ## Power SFT.R.sq slope truncated.R.sq mean.k. median.k. max.k. ## 1 1 0.0133 -0.203 -0.25200 83.8 76.00 133.0 ## 2 2 0.1260 -3.130 -0.12400 43.9 31.70 94.1 ## 3 3 0.1240 -2.570 -0.10400 30.1 16.60 78.5 ## 4 4 0.1920 -3.680 -0.03720 24.1 11.00 71.2 ## 5 5 0.2060 -3.590 -0.01890 21.0 7.74 67.5 ## 6 6 0.2390 -4.180 0.02530 19.2 6.00 65.3 ## 7 7 0.1910 -3.630 -0.02780 18.1 4.61 64.0 ## 8 8 0.2550 -4.350 0.05880 17.3 3.68 63.2 ## 9 9 0.1990 -3.710 -0.01210 16.8 3.06 62.6 ## 10 10 0.2090 -3.740 -0.00272 16.4 2.40 62.2 dev.off() ## png ## 2 6.1.2 Set Main Parameters #set your parameters power= 4 minModuleSize = 20 metadata_binary= design hub_threshold= 0.85 GS_threshold= 0.85 #(.2) default #mapping_file= read.table(&quot;resources/Metabolites-HMDB.csv&quot;, header = T) 6.1.3 WGCNA Main Function Run_WGCNA= function(data, metadata_binary, power,minModuleSize, hub_threshold ,GS_threshold, mapping_file=NULL){ power= power minModuleSize= minModuleSize data[]= lapply(data, as.numeric) datExpr= t(data) #so that samples become in row #This function checks data for missing entries, entries with weights below a threshold, and zero-variance genes, goods &lt;- goodSamplesGenes(datExpr, verbose = 3) datExpr= datExpr[goods$goodSamples== TRUE, goods$goodGenes == TRUE ] net = blockwiseModules(datExpr, corType = &quot;pearson&quot;, maxBlockSize = 5000, networkType = &quot;signed&quot;, power = power, minModuleSize =minModuleSize, mergeCutHeight = 0.25, numericLabels = F, saveTOMs = TRUE, pamRespectsDendro = FALSE, saveTOMFileBase = &quot;TOM&quot;) print(&quot;module detection is done&quot;) # A data frame with module eigengenes module_eigengenes &lt;- net$MEs write.csv(module_eigengenes, &quot;output/module_eigengenes.csv&quot;) names(module_eigengenes)= gsub(&quot;ME&quot;, &quot;&quot;, names(module_eigengenes) ) print(&quot;module_eigengenes file is saved&quot;) #gene module assignment module.gene.assign= net$colors #vector of colors that assign each gene to its corresponding module write.csv(module.gene.assign, &quot;output/gene_module_assignment.csv&quot;) print(&quot;gene_module_assignment file is saved&quot;) #Extract proteins for each modules module_names= names(module_eigengenes) module_lists= list() for (i in module_names){ module_lists[[i]]= module.gene.assign[module.gene.assign== i] |&gt; names() } df &lt;- ldply(module_lists, rbind) |&gt; t() df[is.na(df)] &lt;- &quot;&quot; df= as.data.frame(df) names(df) = df[1,] |&gt; as.vector() modules= df[-1,] write.csv(modules, paste0(&quot;output/&quot;, &quot;modules.csv&quot;), row.names = FALSE) print(&quot;modules file is saved!&quot;) #calculate gene significance (a measure that show to what degree the gene is correlated with phenotype) nSamples &lt;- nrow(datExpr) nGenes &lt;- ncol(datExpr) gene.signf.corr &lt;- cor(datExpr, metadata_binary, use = &#39;p&#39;) gene.signf.corr.pvals &lt;- corPvalueStudent(gene.signf.corr, nSamples) write.csv(gene.signf.corr, &quot;output/gene.trait.corr.csv&quot;) gene.signf.corr= gene.signf.corr |&gt; as.data.frame() #Module membership (reflects intra-modular connectivity) # calculate the module membership values node i. The module membership kME(q) specifies how close node i is to module q. #KME (module membership) defined as correlation between gene expression profile and the module eigengene of certain module # if MMblue(i) is close to 1 or -1, it is highly connected to the blue module genes. # The sign of module membership encodes whether the gene has a positive or a negative relationship with the module eigengene. datKME = signedKME(datExpr, module_eigengenes) write.csv(datKME, &quot;output/module_membership.csv&quot;) #extract hub genes #Extract hubs for each modules module_names= names(module_eigengenes) hub_lists &lt;- vector(&quot;list&quot;, length(module_names)) names(hub_lists)= names(module_eigengenes) threshold= hub_threshold for(j in 1:length(module_names)){ hub_mod= row.names(datKME)[datKME[,j] &gt; threshold] hub_lists[[j]]= intersect(unlist(modules[j]), hub_mod) } df= ldply(hub_lists, rbind) |&gt; t() df[is.na(df)] &lt;- &quot;&quot; df= as.data.frame(df) names(df) = paste0(unlist(df[1,]), &quot;_hub&quot;) hubs= df[-1,] write.csv(hubs, paste0(&quot;output/&quot;, &quot;module_hubs.csv&quot;), row.names = FALSE) print(&quot;hubs file is saved!&quot;) #Extract hubs correlated with phenotype hub_pheno &lt;- vector(&quot;list&quot;, length= ncol(gene.signf.corr)) names(hub_pheno)= names(gene.signf.corr) GS_threshold= GS_threshold for(j in 1:ncol(gene.signf.corr)){ all_hubs= hub_lists |&gt; unlist() all_hubs= all_hubs[all_hubs !=&quot;&quot; &amp; !is.na(all_hubs)] pheno_h= row.names(gene.signf.corr)[gene.signf.corr[,j] &gt; GS_threshold] pheno_h= pheno_h[pheno_h %in% all_hubs] hub_pheno[[j]] = pheno_h } df= ldply(hub_pheno, rbind) |&gt; t() df[is.na(df)] &lt;- &quot;&quot; df= as.data.frame(df) names(df) = paste0(unlist(df[1,]), &quot;_hub&quot;) hubs= df[-1,] #write.csv(hubs, paste0(&quot;output/&quot;, &quot;phenotype_cor_hubs.csv&quot;), row.names = FALSE) #print(&quot;hubs_pheno file is saved!&quot;) #export networks for cytoscape module_names= names(module_eigengenes) for( i in module_names) { datexpr_mod = datExpr[,module.gene.assign == i] TOM_mod = TOMsimilarityFromExpr(datexpr_mod, power = power, networkType = &quot;signed&quot;, TOMType=&quot;signed Nowick&quot;); genes = colnames(datexpr_mod) dimnames(TOM_mod) = list(genes, genes) # nTop = 30; # IMConn = softConnectivity(datExpr[, probes]); # top = (rank(-IMConn) &lt;= nTop) cyt = exportNetworkToCytoscape(TOM_mod, edgeFile = paste(&quot;output/CytoscapeInput-edges-.02tom&quot;, paste(i, collapse=&quot;-&quot;), &quot;.txt&quot;, sep=&quot;&quot;), #nodeFile = paste(&quot;output/CytoscapeInput-nodes-&quot;, paste(i, collapse=&quot;-&quot;), &quot;.txt&quot;, sep=&quot;&quot;), weighted = TRUE, threshold = 0.02, #threshold for including edges in the output, Default is 0.02 nodeNames = genes, altNodeNames = genes); } print(&quot;networks exported!&quot;) print(&quot;ALL iS DONE!&quot;) return(net) } 6.1.4 Run WGCNA res= Run_WGCNA(data, metadata_binary=metadata_binary, power=power, minModuleSize= minModuleSize, hub_threshold= hub_threshold, GS_threshold=GS_threshold, mapping_file=NULL) ## Flagging genes and samples with too many missing values... ## ..step 1 ## [1] &quot;module detection is done&quot; ## [1] &quot;module_eigengenes file is saved&quot; ## [1] &quot;gene_module_assignment file is saved&quot; ## [1] &quot;modules file is saved!&quot; ## [1] &quot;hubs file is saved!&quot; ## TOM calculation: adjacency.. ## ..will not use multithreading. ## Fraction of slow calculations: 0.000000 ## ..connectivity.. ## ..matrix multiplication (system BLAS).. ## ..normalization.. ## ..done. ## TOM calculation: adjacency.. ## ..will not use multithreading. ## Fraction of slow calculations: 0.000000 ## ..connectivity.. ## ..matrix multiplication (system BLAS).. ## ..normalization.. ## ..done. ## TOM calculation: adjacency.. ## ..will not use multithreading. ## Fraction of slow calculations: 0.000000 ## ..connectivity.. ## ..matrix multiplication (system BLAS).. ## ..normalization.. ## ..done. ## TOM calculation: adjacency.. ## ..will not use multithreading. ## Fraction of slow calculations: 0.000000 ## ..connectivity.. ## ..matrix multiplication (system BLAS).. ## ..normalization.. ## ..done. ## [1] &quot;networks exported!&quot; ## [1] &quot;ALL iS DONE!&quot; ## R version 4.4.1 (2024-06-14 ucrt) ## Platform: x86_64-w64-mingw32/x64 ## Running under: Windows 10 x64 (build 19045) ## ## Matrix products: default ## ## ## locale: ## [1] LC_COLLATE=English_United States.utf8 ## [2] LC_CTYPE=English_United States.utf8 ## [3] LC_MONETARY=English_United States.utf8 ## [4] LC_NUMERIC=C ## [5] LC_TIME=English_United States.utf8 ## ## time zone: Africa/Cairo ## tzcode source: internal ## ## attached base packages: ## [1] grid stats graphics grDevices utils datasets methods ## [8] base ## ## other attached packages: ## [1] WGCNA_1.72-5 fastcluster_1.2.6 dynamicTreeCut_1.63-1 ## [4] ggrepel_0.9.6 viridis_0.6.5 fields_16.2 ## [7] viridisLite_0.4.2 spam_2.10-0 biomaRt_2.61.2 ## [10] ComplexHeatmap_2.21.0 circlize_0.4.16 RColorBrewer_1.1-3 ## [13] memoise_2.0.1 caret_6.0-94 lattice_0.22-6 ## [16] pls_2.8-3 Rserve_1.8-13 MetaboAnalystR_3.2.0 ## [19] cowplot_1.1.3 DT_0.33 openxlsx_4.2.6.1 ## [22] lubridate_1.9.3 forcats_1.0.0 stringr_1.5.1 ## [25] purrr_1.0.2 readr_2.1.5 tidyr_1.3.1 ## [28] ggplot2_3.5.1 tidyverse_2.0.0 dplyr_1.1.4 ## [31] plyr_1.8.9 tibble_3.2.1 ## ## loaded via a namespace (and not attached): ## [1] splines_4.4.1 filelock_1.0.3 bitops_1.0-7 ## [4] preprocessCore_1.67.0 hardhat_1.4.0 pROC_1.18.5 ## [7] rpart_4.1.23 httr2_1.0.2 lifecycle_1.0.4 ## [10] edgeR_4.3.5 doParallel_1.0.17 globals_0.16.3 ## [13] MASS_7.3-60.2 scrime_1.3.5 crosstalk_1.2.1 ## [16] backports_1.5.0 magrittr_2.0.3 Hmisc_5.1-3 ## [19] limma_3.61.5 plotly_4.10.4 sass_0.4.9 ## [22] rmarkdown_2.27 jquerylib_0.1.4 yaml_2.3.10 ## [25] zip_2.3.1 DBI_1.2.3 maps_3.4.2 ## [28] zlibbioc_1.50.0 BiocGenerics_0.52.0 nnet_7.3-19 ## [31] rappdirs_0.3.3 ipred_0.9-15 GenomeInfoDbData_1.2.12 ## [34] lava_1.8.0 IRanges_2.38.1 S4Vectors_0.42.1 ## [37] listenv_0.9.1 ellipse_0.5.0 parallelly_1.38.0 ## [40] codetools_0.2-20 xml2_1.3.6 RApiSerialize_0.1.3 ## [43] tidyselect_1.2.1 shape_1.4.6.1 UCSC.utils_1.1.0 ## [46] farver_2.1.2 base64enc_0.1-3 BiocFileCache_2.13.0 ## [49] matrixStats_1.3.0 stats4_4.4.1 jsonlite_1.8.8 ## [52] GetoptLong_1.0.5 multtest_2.61.0 e1071_1.7-14 ## [55] Formula_1.2-5 survival_3.6-4 iterators_1.0.14 ## [58] systemfonts_1.2.3 foreach_1.5.2 progress_1.2.3 ## [61] tools_4.4.1 ragg_1.3.2 Rcpp_1.0.13 ## [64] glue_1.7.0 gridExtra_2.3 prodlim_2024.06.25 ## [67] xfun_0.46 GenomeInfoDb_1.41.1 crmn_0.0.21 ## [70] withr_3.0.0 fastmap_1.2.0 caTools_1.18.2 ## [73] digest_0.6.36 timechange_0.3.0 R6_2.5.1 ## [76] textshaping_0.4.0 colorspace_2.1-1 GO.db_3.19.1 ## [79] Cairo_1.6-2 gtools_3.9.5 RSQLite_2.3.7 ## [82] generics_0.1.3 data.table_1.15.4 recipes_1.1.0 ## [85] class_7.3-22 prettyunits_1.2.0 httr_1.4.7 ## [88] htmlwidgets_1.6.4 ModelMetrics_1.2.2.2 pkgconfig_2.0.3 ## [91] gtable_0.3.5 timeDate_4032.109 blob_1.2.4 ## [94] siggenes_1.79.0 impute_1.79.0 XVector_0.44.0 ## [97] htmltools_0.5.8.1 dotCall64_1.1-1 bookdown_0.40 ## [100] fgsea_1.31.0 clue_0.3-65 scales_1.3.0 ## [103] Biobase_2.64.0 png_0.1-8 gower_1.0.1 ## [106] knitr_1.48 rstudioapi_0.16.0 tzdb_0.4.0 ## [109] reshape2_1.4.4 rjson_0.2.21 checkmate_2.3.1 ## [112] curl_5.2.1 nlme_3.1-164 proxy_0.4-27 ## [115] cachem_1.1.0 GlobalOptions_0.1.2 KernSmooth_2.23-24 ## [118] parallel_4.4.1 foreign_0.8-86 AnnotationDbi_1.67.0 ## [121] pillar_1.11.0 vctrs_0.6.5 gplots_3.1.3.1 ## [124] pcaMethods_1.97.0 stringfish_0.16.0 dbplyr_2.5.0 ## [127] cluster_2.1.6 htmlTable_2.4.3 evaluate_0.24.0 ## [130] magick_2.8.4 cli_3.6.3 locfit_1.5-9.10 ## [133] compiler_4.4.1 rlang_1.1.4 crayon_1.5.3 ## [136] future.apply_1.11.2 labeling_0.4.3 stringi_1.8.4 ## [139] BiocParallel_1.39.0 munsell_0.5.1 Biostrings_2.72.1 ## [142] lazyeval_0.2.2 Matrix_1.7-0 hms_1.1.3 ## [145] glasso_1.11 bit64_4.0.5 future_1.33.2 ## [148] KEGGREST_1.45.1 statmod_1.5.0 highr_0.11 ## [151] qs_0.26.3 igraph_2.0.3 RcppParallel_5.1.8 ## [154] bslib_0.8.0 fastmatch_1.1-4 bit_4.0.5 "],["wgcna-visualizations.html", "Section 7 WGCNA Visualizations 7.1 Module-phenotype Correlation Plot 7.2 Samples Dendrogram Plot 7.3 Cluster Dendrogram Plot 7.4 TOM plot (Heatmap of Correlated Features Across All Modules) 7.5 MDS plot 7.6 TOM plot for Selected Features 7.7 Module Membership Vs Gene Significance Scatter plot", " Section 7 WGCNA Visualizations Description: This pipeline performs all visualizations for the WGCNA pipeline. Project Initialization #Sets the working directory and creates subfolders for organizing outputs. mypath= &quot;C:/Users/USER/Documents/Github/CRC_project/&quot; dir.create(&quot;output&quot;) dir.create(&quot;plots&quot;) dir.create(&quot;input&quot;) #load packages library(gtools) library(pROC) library(ape) library(ggdendro) library(WGCNA) library(stats) library(flashClust) library(plyr) library(dplyr) library(tidyr) library(purrr) library(tibble) library(tidyverse) library(gridExtra) library(gplots) library(ggplot2) library(circlize) library(ComplexHeatmap) allowWGCNAThreads() ## Allowing multi-threading with up to 4 threads. Prepare Data #load data dir= paste0(mypath, &quot;output/&quot;) # expression data data= read.csv(paste0(mypath,&quot;input/data_for_downstream.csv&quot;)) data = data |&gt; column_to_rownames(colnames(data)[1]) # differential expression file de= read.csv(paste0(mypath, &quot;output/DE_sig.csv&quot;)) de=de |&gt; filter(abs(logFC) &gt;= log2(3)) module_eigengenes= read.csv(paste0(mypath,&quot;output/module_eigengenes.csv&quot;)) |&gt; column_to_rownames(&quot;X&quot;) group_dist= gsub(&quot;_.*&quot;, &quot;&quot;, colnames(data)) group_levels= unique(group_dist) group_colors &lt;- c(&quot;#fc8d62&quot;, &quot;#66c2a5&quot;) names(group_colors) &lt;- group_levels #change type to numeric data[]= lapply(data, as.numeric) datExpr= t(data) #samples become in row #This function checks data for missing entries, entries with weights below a threshold, and zero-variance genes, goods &lt;- goodSamplesGenes(datExpr, verbose = 3) ## Flagging genes and samples with too many missing values... ## ..step 1 datExpr= datExpr[goods$goodSamples== TRUE, goods$goodGenes == TRUE ] Prepare Metadata #metadata metadata= data.frame(sample= colnames(data) , condition= group_dist, cond_binary= as.numeric(binarizeCategoricalColumns(group_dist, levelOrder= list(group_levels[2], group_levels[1]))[[1]])) #metadata= read.csv(&quot;input/metadata_test.csv&quot; ) metadata = metadata |&gt; column_to_rownames(&quot;sample&quot;) metadata$condition= factor(metadata$condition, levels = group_levels) design= model.matrix(~ 0+condition , metadata) head(design) ## conditionCRC conditionCtrl ## CRC_01 1 0 ## CRC_02 1 0 ## CRC_03 1 0 ## CRC_04 1 0 ## CRC_05 1 0 ## CRC_06 1 0 Set parameters and run main WGCNA function #parameters for WGCNA power= 4 minModuleSize = 20 metadata_binary= design #Run WGCNA net = blockwiseModules(datExpr, corType = &quot;pearson&quot;, maxBlockSize = 5000, networkType = &quot;signed&quot;, power = power, minModuleSize =minModuleSize, mergeCutHeight = 0.25, numericLabels = F, saveTOMs = TRUE, pamRespectsDendro = FALSE, saveTOMFileBase = &quot;TOM&quot;) 7.1 Module-phenotype Correlation Plot #Module trait correlation (visualize which module associated with what phenotype) #We apply pearson correlation between metadata traits (conditions) and module eigen gene (1st principal component of module genes/metabolites) plotheatmap=function(datExpr, design, module_eigengenes){ traits= design |&gt; as.data.frame() head(traits) # Define numbers of genes and samples nSamples &lt;- nrow(datExpr) nGenes &lt;- ncol(datExpr) module.trait.corr &lt;- WGCNA::cor(module_eigengenes, traits, use = &#39;p&#39;) module.trait.corr.pvals &lt;- corPvalueStudent(module.trait.corr, nSamples) #module_trait heatmap of WGCNA package # correlations and their p-values textMatrix = paste(signif(module.trait.corr, 2), &quot;\\n(&quot;, signif(module.trait.corr.pvals, 1), &quot;)&quot;, sep = &quot;&quot;) dim(textMatrix) = dim(module.trait.corr) par(mar = c(6, 6, 4, 6)) color= colorpanel(250, &quot;#667F9C&quot;, &quot;white&quot;, &quot;#FE1B1B&quot;) #color= greenWhiteRed(50) # Display the correlation values within a heatmap plot labeledHeatmap(Matrix = module.trait.corr, xLabels = gsub(&quot;condition&quot;, &quot;&quot;, names(traits)), yLabels = names(module_eigengenes), ySymbols = names(module_eigengenes), colorLabels = FALSE, colors = color, textMatrix = textMatrix, setStdMargins = T, cex.text = 0.8, zlim = c(-1, 1),xColorWidth = 1 * strheight(&quot;M&quot;), yColorWidth = 1.5 * strwidth(&quot;M&quot;),xColorOffset = strheight(&quot;M&quot;)/6, yColorOffset = strwidth(&quot;M&quot;)/6, font.lab.x = 2, cex.legendLabel = 2, font.lab.y = 2, xLabelsAngle = 75, main = paste(&quot;Module-Condition Relationship&quot;), plotLegend= TRUE) } #Display plotheatmap(datExpr, design, module_eigengenes) #Save png(&quot;plots/heatmap_module_phenotype_cor.png&quot;, width = 4000, height = 4500, res= 600) plotheatmap(datExpr, design, module_eigengenes) dev.off() ## png ## 2 7.2 Samples Dendrogram Plot #Hierarchical clustering of samples, detect outlier samples, #and association of sample with certain trait plotDendroAndphenotype= function(data, metadata){ #Build adjacency matrix for samples A = adjacency(data, type = &quot;distance&quot;) # this calculates the whole network connectivity k = as.numeric(apply(A, 2, sum)) - 1 # standardized connectivity Z.k = scale(k) # Designate samples as outlying if their Z.k value is below the threshold thresholdZ.k = -5 # often -2.5 # the color vector indicates outlyingness (red) outlierColor = ifelse(Z.k &lt; thresholdZ.k, &quot;red&quot;, &quot;black&quot;) # calculate the cluster tree using flahsClust or hclust sampleTree = flashClust(as.dist(1 - A), method = &quot;average&quot;) # Convert traits to a color representation: where red indicates high # values traitColors = data.frame(numbers2colors(as.numeric(metadata$cond_binary), signed = TRUE)) #dimnames(traitColors)[[2]] = &quot;Inflammation_lvl&quot; datColors = data.frame(outlier_Samples = outlierColor, Condition= traitColors) colnames(datColors)[2]= &quot;Conditions&quot; # Plot the sample dendrogram and the colors underneath. WGCNA::plotDendroAndColors(sampleTree, groupLabels = names(datColors), colors = datColors, cex.rowText = 5, main = &quot;Sample dendrogram and Homogeneity of samples heatmap&quot;) } # Display plotDendroAndphenotype(data,metadata) # Save png(paste0(mypath,&quot;plots/WGCNA_dendrogram.png&quot;), width = 8000, height = 6000, res= 600) plotDendroAndphenotype(data, metadata) dev.off() ## png ## 2 7.3 Cluster Dendrogram Plot plotDendro= function(){ plotDendroAndColors(net$dendrograms[[1]], net$colors, paste0(&quot;Merged\\n&quot;, &quot;Modules&quot;), dendroLabels = FALSE, addGuide = TRUE, hang= 0.03, cex.colorLabels = 0.6, guideHang = 0.05) } # Display plotDendro() # Save png(paste0(mypath,&quot;plots/dendrogram_merged_modules.png&quot;), width = 2200, height = 2500, res= 600) plotDendro() dev.off() ## png ## 2 #for error, Error in .plotOrderedColorSubplot(order = order, colors = colors, rowLabels = rowLabels, : #Length of colors vector not compatible with number of objects in &#39;order&#39;. #set good samples and good genes 7.4 TOM plot (Heatmap of Correlated Features Across All Modules) dissTOM= 1 - TOMsimilarityFromExpr(datExpr, power= power) #datExpr samples in rows ## TOM calculation: adjacency.. ## ..will not use multithreading. ## Fraction of slow calculations: 0.000000 ## ..connectivity.. ## ..matrix multiplication (system BLAS).. ## ..normalization.. ## ..done. dendro= net$dendrograms[[1]] moduleColorsAutomatic= net$colors plotTOMheatmap = function() { #myheatcol = colorpanel(250,&#39;gold&#39;,&quot;orange&quot;,&#39;darkred&#39;) myheatcol = colorpanel(250,&#39;red&#39;,&quot;orange&quot;,&#39;lemonchiffon&#39;) # Transform dissTOM with a power to enhance visibility TOMplot(dissTOM, dendro, moduleColorsAutomatic,col= myheatcol, main = &quot;Module Heatmap Plot, All Features&quot;) } # Display plotTOMheatmap() # save png(paste0(mypath,&quot;plots/TOM_PLOT_module_heatmap_all.png&quot;), width = 800, height = 600) plotTOMheatmap() dev.off() ## png ## 2 7.5 MDS plot dissTOM= 1 - TOMsimilarityFromExpr(datExpr, power= 5) ## TOM calculation: adjacency.. ## ..will not use multithreading. ## Fraction of slow calculations: 0.000000 ## ..connectivity.. ## ..matrix multiplication (system BLAS).. ## ..normalization.. ## ..done. cmd1=cmdscale(as.dist(dissTOM),2) # Display plot(cmd1,col=moduleColorsAutomatic,main=&quot;MDS plot&quot;, xlab=&quot;Scaling Dimension 1&quot;,ylab=&quot;Scaling Dimension 2&quot;) # Save png(paste0(mypath,&quot;plots/MDS_plot.png&quot;), width = 2800, height = 3300, res= 600) par(mfrow=c(1,1)) plot(cmd1,col=moduleColorsAutomatic,main=&quot;MDS plot&quot;, xlab=&quot;Scaling Dimension 1&quot;,ylab=&quot;Scaling Dimension 2&quot;) dev.off() ## png ## 2 7.6 TOM plot for Selected Features # Get Differentially expressed features genes= de$X # subset data to have only selected proteins datExpr_subset= datExpr[,colnames(datExpr) %in% genes] dissTOM_subset= 1 - TOMsimilarityFromExpr(datExpr_subset, power= power) ## TOM calculation: adjacency.. ## ..will not use multithreading. ## Fraction of slow calculations: 0.000000 ## ..connectivity.. ## ..matrix multiplication (system BLAS).. ## ..normalization.. ## ..done. dendro_subset = hclust(as.dist(dissTOM_subset), method = &quot;average&quot;) module.gene.assign= net$colors moduleColors_subset= module.gene.assign[names(module.gene.assign)%in% genes] png(paste0(mypath,&quot;plots/module_heatmap_TOM_PLOT_selected.png&quot;), width = 2800, height = 3300, res= 600) #myheatcol = colorpanel(250,&#39;gold&#39;,&quot;orange&quot;,&#39;darkred&#39;) myheatcol = colorpanel(250,&#39;red&#39;,&quot;orange&quot;,&#39;lemonchiffon&#39;) # Transform dissTOM with a power to enhance visibility TOMplot(dissTOM_subset, dendro_subset, moduleColors_subset,col= myheatcol, main = &quot;Module Heatmap Plot&quot;) dev.off() ## png ## 2 # Intramodular analysis: identifying genes with high Gene Significance (GS), i.e., strong association with the phenotype, #and high Module Membership (MM), i.e., strong correlation with the module eigengene (first principal component). datKME = signedKME(datExpr, module_eigengenes) GS.lvl= read.csv(paste0(mypath,&quot;output/gene.trait.corr.csv&quot;)) |&gt; column_to_rownames(&quot;X&quot;) modules_df= read.csv(paste0(mypath,&quot;output/modules.csv&quot;)) module.assign= read.csv(paste0(mypath,&quot;output/gene_module_assignment.csv&quot;)) |&gt; deframe() |&gt; unlist() dim(datKME) ## [1] 263 4 dim(GS.lvl) ## [1] 263 2 7.7 Module Membership Vs Gene Significance Scatter plot # Define plotting function plot_MM_vs_GS &lt;- function() { colorOfColumn &lt;- substring(names(datKME), 4) selectModules &lt;- colnames(modules_df) # Set layout: 2 rows, N/2 columns par(mar = c(5, 4, 4, 2) + 0.1) par(mfrow = c(2, ceiling(length(selectModules) / 2))) for (module in selectModules) { if (!(module %in% module.assign)) { message(paste(&quot;Skipping module&quot;, module, &quot;- not found in module assignment&quot;)) next } column &lt;- match(module, colorOfColumn) restModule &lt;- moduleColorsAutomatic == module if (sum(restModule) &gt; 0) { verboseScatterplot( datKME[restModule, column], GS.lvl[restModule, 1], xlab = paste(&quot;Module Membership\\n&quot;, module, &quot;module&quot;), ylab = &quot;Feature Significance&quot;, main = paste(&quot;kME:&quot;, module, &quot;vs Feature Sig.&quot;), col = module ) } else { message(paste(&quot;No matching entries for module:&quot;, module)) } } } #Display plot_MM_vs_GS() #Save png(paste0(mypath,&quot;plots/mm_vs_sig.png&quot;), width = 800, height = 700) plot_MM_vs_GS() dev.off() ## png ## 2 ## R version 4.4.1 (2024-06-14 ucrt) ## Platform: x86_64-w64-mingw32/x64 ## Running under: Windows 10 x64 (build 19045) ## ## Matrix products: default ## ## ## locale: ## [1] LC_COLLATE=English_United States.utf8 ## [2] LC_CTYPE=English_United States.utf8 ## [3] LC_MONETARY=English_United States.utf8 ## [4] LC_NUMERIC=C ## [5] LC_TIME=English_United States.utf8 ## ## time zone: Africa/Cairo ## tzcode source: internal ## ## attached base packages: ## [1] grid stats graphics grDevices utils datasets methods ## [8] base ## ## other attached packages: ## [1] gplots_3.1.3.1 gridExtra_2.3 flashClust_1.01-2 ## [4] ggdendro_0.2.0 ape_5.8 pROC_1.18.5 ## [7] gtools_3.9.5 WGCNA_1.72-5 fastcluster_1.2.6 ## [10] dynamicTreeCut_1.63-1 ggrepel_0.9.6 viridis_0.6.5 ## [13] fields_16.2 viridisLite_0.4.2 spam_2.10-0 ## [16] biomaRt_2.61.2 ComplexHeatmap_2.21.0 circlize_0.4.16 ## [19] RColorBrewer_1.1-3 memoise_2.0.1 caret_6.0-94 ## [22] lattice_0.22-6 pls_2.8-3 Rserve_1.8-13 ## [25] MetaboAnalystR_3.2.0 cowplot_1.1.3 DT_0.33 ## [28] openxlsx_4.2.6.1 lubridate_1.9.3 forcats_1.0.0 ## [31] stringr_1.5.1 purrr_1.0.2 readr_2.1.5 ## [34] tidyr_1.3.1 ggplot2_3.5.1 tidyverse_2.0.0 ## [37] dplyr_1.1.4 plyr_1.8.9 tibble_3.2.1 ## ## loaded via a namespace (and not attached): ## [1] matrixStats_1.3.0 bitops_1.0-7 httr_1.4.7 ## [4] doParallel_1.0.17 tools_4.4.1 backports_1.5.0 ## [7] R6_2.5.1 lazyeval_0.2.2 GetoptLong_1.0.5 ## [10] withr_3.0.0 prettyunits_1.2.0 preprocessCore_1.67.0 ## [13] cli_3.6.3 Biobase_2.64.0 textshaping_0.4.0 ## [16] Cairo_1.6-2 labeling_0.4.3 sass_0.4.9 ## [19] proxy_0.4-27 systemfonts_1.2.3 foreign_0.8-86 ## [22] siggenes_1.79.0 parallelly_1.38.0 scrime_1.3.5 ## [25] maps_3.4.2 limma_3.61.5 rstudioapi_0.16.0 ## [28] impute_1.79.0 RSQLite_2.3.7 generics_0.1.3 ## [31] shape_1.4.6.1 RApiSerialize_0.1.3 crmn_0.0.21 ## [34] crosstalk_1.2.1 zip_2.3.1 GO.db_3.19.1 ## [37] Matrix_1.7-0 S4Vectors_0.42.1 lifecycle_1.0.4 ## [40] yaml_2.3.10 edgeR_4.3.5 recipes_1.1.0 ## [43] BiocFileCache_2.13.0 blob_1.2.4 crayon_1.5.3 ## [46] KEGGREST_1.45.1 magick_2.8.4 pillar_1.11.0 ## [49] knitr_1.48 fgsea_1.31.0 rjson_0.2.21 ## [52] future.apply_1.11.2 codetools_0.2-20 fastmatch_1.1-4 ## [55] glue_1.7.0 pcaMethods_1.97.0 data.table_1.15.4 ## [58] vctrs_0.6.5 png_0.1-8 gtable_0.3.5 ## [61] cachem_1.1.0 gower_1.0.1 xfun_0.46 ## [64] prodlim_2024.06.25 survival_3.6-4 timeDate_4032.109 ## [67] iterators_1.0.14 hardhat_1.4.0 lava_1.8.0 ## [70] statmod_1.5.0 ipred_0.9-15 nlme_3.1-164 ## [73] bit64_4.0.5 progress_1.2.3 filelock_1.0.3 ## [76] GenomeInfoDb_1.41.1 bslib_0.8.0 KernSmooth_2.23-24 ## [79] rpart_4.1.23 colorspace_2.1-1 BiocGenerics_0.52.0 ## [82] DBI_1.2.3 Hmisc_5.1-3 nnet_7.3-19 ## [85] tidyselect_1.2.1 bit_4.0.5 compiler_4.4.1 ## [88] curl_5.2.1 httr2_1.0.2 htmlTable_2.4.3 ## [91] xml2_1.3.6 plotly_4.10.4 stringfish_0.16.0 ## [94] bookdown_0.40 checkmate_2.3.1 scales_1.3.0 ## [97] caTools_1.18.2 rappdirs_0.3.3 digest_0.6.36 ## [100] rmarkdown_2.27 XVector_0.44.0 htmltools_0.5.8.1 ## [103] pkgconfig_2.0.3 base64enc_0.1-3 highr_0.11 ## [106] dbplyr_2.5.0 fastmap_1.2.0 rlang_1.1.4 ## [109] GlobalOptions_0.1.2 htmlwidgets_1.6.4 UCSC.utils_1.1.0 ## [112] farver_2.1.2 jquerylib_0.1.4 jsonlite_1.8.8 ## [115] BiocParallel_1.39.0 ModelMetrics_1.2.2.2 magrittr_2.0.3 ## [118] Formula_1.2-5 GenomeInfoDbData_1.2.12 dotCall64_1.1-1 ## [121] munsell_0.5.1 Rcpp_1.0.13 stringi_1.8.4 ## [124] zlibbioc_1.50.0 MASS_7.3-60.2 parallel_4.4.1 ## [127] listenv_0.9.1 Biostrings_2.72.1 splines_4.4.1 ## [130] multtest_2.61.0 hms_1.1.3 locfit_1.5-9.10 ## [133] igraph_2.0.3 reshape2_1.4.4 stats4_4.4.1 ## [136] evaluate_0.24.0 RcppParallel_5.1.8 tzdb_0.4.0 ## [139] foreach_1.5.2 qs_0.26.3 future_1.33.2 ## [142] clue_0.3-65 e1071_1.7-14 glasso_1.11 ## [145] class_7.3-22 ragg_1.3.2 AnnotationDbi_1.67.0 ## [148] ellipse_0.5.0 IRanges_2.38.1 cluster_2.1.6 ## [151] timechange_0.3.0 globals_0.16.3 "],["module-eigengene-differentiation.html", "Section 8 Module Eigengene Differentiation 8.1 Function for Module eigengene differentiation 8.2 Horizontal Bar plot 8.3 Chord plot of hubgenes and corresponding modules", " Section 8 Module Eigengene Differentiation Description: This pipeline performs post-WGCNA analysis by testing whether module eigengenes significantly differentiate between phenotypic groups.It uses adaptive statistical testing (parametric or non-parametric) based on normality and number of groups to assess eigengene–phenotype associations. And The results are corrected for multiple testing using FDR Benjamini-Hochberg test. Project Initialization #Sets the working directory and creates subfolders for organizing outputs. mypath= &quot;C:/Users/USER/Documents/Github/CRC_project/&quot; dir.create(&quot;output&quot;) dir.create(&quot;plots&quot;) dir.create(&quot;input&quot;) #load packages library(plyr) library(dplyr) library(tidyr) library(purrr) library(tibble) library(tidyverse) library(gridExtra) library(gplots) library(ggplot2) #load data module_eigengenes= read.csv(paste0(mypath,&quot;output/module_eigengenes.csv&quot;)) %&gt;% column_to_rownames(&quot;X&quot;) data= read.csv(paste0(mypath,&quot;input/data_for_downstream.csv&quot;)) data = data %&gt;% column_to_rownames(colnames(data)[1]) dir= paste0(mypath, &quot;output/&quot;) group_dist= gsub(&quot;_.*&quot;, &quot;&quot;, colnames(data)) group_levels= unique(group_dist) 8.1 Function for Module eigengene differentiation # A function that apply the appropriate statistical test based on # normality and number of groups to assess differential expression # of module eigengenes (first principal components) between groups. test_differentiation &lt;- function(module_eigengenes, group_vector, output_path = NULL) { check_normality &lt;- function(vector) { x &lt;- as.numeric(vector) x &lt;- x[!is.na(x)] if (length(x) &lt;= 3) { return(&quot;Non-parametric&quot;) } result &lt;- tryCatch(shapiro.test(x), error = function(e) return(NULL)) if (is.null(result)) return(&quot;Non-parametric&quot;) if (result$p.value &gt; 0.05) &quot;Parametric&quot; else &quot;Non-parametric&quot; } # Prepare group information group_vector &lt;- as.factor(group_vector) group_levels &lt;- levels(group_vector) n_groups &lt;- length(group_levels) mods &lt;- colnames(module_eigengenes) results_list &lt;- list() for (mod in mods) { x &lt;- module_eigengenes[[mod]] test_type &lt;- check_normality(x) if (n_groups == 2) { if (test_type == &quot;Parametric&quot;) { res &lt;- t.test(x ~ group_vector) test_used &lt;- &quot;t-test&quot; } else { res &lt;- wilcox.test(x ~ group_vector) test_used &lt;- &quot;Wilcoxon&quot; } p &lt;- res$p.value } else if (n_groups &gt; 2) { if (test_type == &quot;Parametric&quot;) { res &lt;- aov(x ~ group_vector) p &lt;- summary(res)[[1]][[&quot;Pr(&gt;F)&quot;]][1] test_used &lt;- &quot;ANOVA&quot; } else { res &lt;- kruskal.test(x ~ group_vector) p &lt;- res$p.value test_used &lt;- &quot;Kruskal-Wallis&quot; } } else { p &lt;- NA test_used &lt;- &quot;Invalid group size&quot; } results_list[[mod]] &lt;- list(p.value = p, test = test_used) } # Convert results to a data frame res_df &lt;- do.call(rbind, lapply(names(results_list), function(mod) { row &lt;- results_list[[mod]] data.frame(modules = mod, p.value = row$p.value, test = row$test) })) # Adjust FDR res_df$fdr &lt;- p.adjust(res_df$p.value, method = &quot;BH&quot;) res_df$sig &lt;- ifelse(res_df$fdr &lt;= 0.05, &quot;***&quot;, &quot;&quot;) res_df$module &lt;- gsub(&quot;ME&quot;, &quot;&quot;, res_df$module) # Save if path is provided if (!is.null(output_path)) { write.csv(res_df,paste0(output_path ,&quot;ME_differentiation.csv&quot;) , row.names = FALSE) } return(res_df) } res= test_differentiation(module_eigengenes, group_dist, dir) head(res) ## modules p.value test fdr sig module ## 1 MEblue 1.082509e-05 Wilcoxon 2.165018e-05 *** blue ## 2 MEbrown 7.959363e-01 Wilcoxon 7.959363e-01 brown ## 3 MEturquoise 1.082509e-05 Wilcoxon 2.165018e-05 *** turquoise ## 4 MEgrey 3.930481e-01 Wilcoxon 5.240642e-01 grey 8.2 Horizontal Bar plot p= ggplot(res) + geom_bar(aes(x = modules, y = -log10(fdr), fill = module), stat = &quot;identity&quot;) + coord_flip() + # Flip coordinates to make the bar plot horizontal scale_fill_identity() + # Use actual colors specified in the data frame geom_text(aes(x = modules , y =-log10(fdr) , label = sig), position = position_dodge(width = 0.9), vjust = -0.5, size=.5 ,color = &quot;black&quot;) + theme_minimal() + labs(y = &quot;-log10(FDR)&quot;, x = &quot;Modules&quot;) + ggtitle(&quot;Module Eigengene Differentiation&quot;) + # Label axes and provide a title theme(legend.position = &quot;none&quot;, plot.title = element_text(hjust = 0.5, size = 12), axis.text = element_text(size=12), axis.title=element_text(size=13) ) + ylim(0,12) + geom_hline(yintercept = -log10(0.05), linetype = &quot;dashed&quot;, color = &quot;black&quot;, lwd= 1) print(p) ggsave(paste0(mypath,&quot;plots/module_differentiation.png&quot;), p,width= 10, height = 7, dpi=900 ) 8.3 Chord plot of hubgenes and corresponding modules module_hubs= read.csv(paste0(mypath,&quot;output/module_hubs.csv&quot;)) colnames(module_hubs)=gsub(&quot;_hub&quot;, &quot;&quot;, names(module_hubs) ) # We can plot only the hub features of significant modules (Module differentiation output) module_sig= read.csv(paste0(mypath,&quot;output/ME_differentiation.csv&quot;)) m.sig= module_sig$module[module_sig$sig== &quot;***&quot;] module_hubs= module_hubs |&gt; dplyr::select(m.sig) #build similarity/ design matrix keydrivers= unlist(module_hubs) |&gt; unique() keydrivers= keydrivers[keydrivers != &quot;&quot; &amp; !is.na(keydrivers)] mtx= matrix(nrow= ncol(module_hubs), ncol = length( keydrivers)) row.names(mtx)= colnames(module_hubs) colnames(mtx)= paste0(keydrivers) #build similarity matrix #colnames of matrix included in keydrivers specified for certain module/ phenotype(rows) then put in 1 mod= apply(module_hubs, 2, function(x) as.list(x)) for (i in seq_along(mod)){ for (j in 1:ncol(mtx)) { if ( colnames(mtx)[j] %in% mod[[i]] ) { mtx[i, j] &lt;- 1 } else { mtx[i, j] &lt;- 0 } } } # make shorter row names colnames(mtx)= substr(colnames(mtx), 1, 25) library(circlize) # Define chord plot function plot_chord_hubs &lt;- function() { par(cex = 0.8, mar = c(1, 1, 1, 1)) circos.par( gap.degree = 1, track.margin = c(0.05, 0.05), canvas.xlim = c(-1.2, 1.2), canvas.ylim = c(-1.2, 1.2), points.overflow.warning = FALSE ) chordDiagram( mtx, annotationTrack = &quot;grid&quot;, transparency = 0.5 ) # Labels customization labels_to_asterisk &lt;- NULL # Example: c(&quot;TP53&quot;, &quot;MYC&quot;) labels_red &lt;- NULL # Example: c(&quot;BRCA1&quot;, &quot;EGFR&quot;) circos.track(track.index = 1, panel.fun = function(x, y) { label &lt;- CELL_META$sector.index modified_label &lt;- ifelse(label %in% labels_to_asterisk, paste0(label, &quot; ***&quot;), label) label_color &lt;- ifelse(label %in% labels_red, &quot;red&quot;, &quot;black&quot;) circos.text( CELL_META$xcenter, CELL_META$cell.ylim[2] * 3.5, modified_label, col = label_color, cex = 0.7, font = 2, facing = &quot;clockwise&quot;, niceFacing = TRUE, adj = c(0, 0) ) }, bg.border = NA) circos.clear() } # Display plot_chord_hubs() # Save png(paste0(mypath,&quot;plots/chord_plot_hubs.png&quot;), width = 9000, height = 9000, res = 600) plot_chord_hubs() dev.off() ## png ## 2 ## R version 4.4.1 (2024-06-14 ucrt) ## Platform: x86_64-w64-mingw32/x64 ## Running under: Windows 10 x64 (build 19045) ## ## Matrix products: default ## ## ## locale: ## [1] LC_COLLATE=English_United States.utf8 ## [2] LC_CTYPE=English_United States.utf8 ## [3] LC_MONETARY=English_United States.utf8 ## [4] LC_NUMERIC=C ## [5] LC_TIME=English_United States.utf8 ## ## time zone: Africa/Cairo ## tzcode source: internal ## ## attached base packages: ## [1] grid stats graphics grDevices utils datasets methods ## [8] base ## ## other attached packages: ## [1] gplots_3.1.3.1 gridExtra_2.3 flashClust_1.01-2 ## [4] ggdendro_0.2.0 ape_5.8 pROC_1.18.5 ## [7] gtools_3.9.5 WGCNA_1.72-5 fastcluster_1.2.6 ## [10] dynamicTreeCut_1.63-1 ggrepel_0.9.6 viridis_0.6.5 ## [13] fields_16.2 viridisLite_0.4.2 spam_2.10-0 ## [16] biomaRt_2.61.2 ComplexHeatmap_2.21.0 circlize_0.4.16 ## [19] RColorBrewer_1.1-3 memoise_2.0.1 caret_6.0-94 ## [22] lattice_0.22-6 pls_2.8-3 Rserve_1.8-13 ## [25] MetaboAnalystR_3.2.0 cowplot_1.1.3 DT_0.33 ## [28] openxlsx_4.2.6.1 lubridate_1.9.3 forcats_1.0.0 ## [31] stringr_1.5.1 purrr_1.0.2 readr_2.1.5 ## [34] tidyr_1.3.1 ggplot2_3.5.1 tidyverse_2.0.0 ## [37] dplyr_1.1.4 plyr_1.8.9 tibble_3.2.1 ## ## loaded via a namespace (and not attached): ## [1] matrixStats_1.3.0 bitops_1.0-7 httr_1.4.7 ## [4] doParallel_1.0.17 tools_4.4.1 backports_1.5.0 ## [7] R6_2.5.1 lazyeval_0.2.2 GetoptLong_1.0.5 ## [10] withr_3.0.0 prettyunits_1.2.0 preprocessCore_1.67.0 ## [13] cli_3.6.3 Biobase_2.64.0 textshaping_0.4.0 ## [16] Cairo_1.6-2 labeling_0.4.3 sass_0.4.9 ## [19] proxy_0.4-27 systemfonts_1.2.3 foreign_0.8-86 ## [22] siggenes_1.79.0 parallelly_1.38.0 scrime_1.3.5 ## [25] maps_3.4.2 limma_3.61.5 rstudioapi_0.16.0 ## [28] impute_1.79.0 RSQLite_2.3.7 generics_0.1.3 ## [31] shape_1.4.6.1 RApiSerialize_0.1.3 crmn_0.0.21 ## [34] crosstalk_1.2.1 zip_2.3.1 GO.db_3.19.1 ## [37] Matrix_1.7-0 S4Vectors_0.42.1 lifecycle_1.0.4 ## [40] yaml_2.3.10 edgeR_4.3.5 recipes_1.1.0 ## [43] BiocFileCache_2.13.0 blob_1.2.4 crayon_1.5.3 ## [46] KEGGREST_1.45.1 magick_2.8.4 pillar_1.11.0 ## [49] knitr_1.48 fgsea_1.31.0 rjson_0.2.21 ## [52] future.apply_1.11.2 codetools_0.2-20 fastmatch_1.1-4 ## [55] glue_1.7.0 pcaMethods_1.97.0 data.table_1.15.4 ## [58] vctrs_0.6.5 png_0.1-8 gtable_0.3.5 ## [61] cachem_1.1.0 gower_1.0.1 xfun_0.46 ## [64] prodlim_2024.06.25 survival_3.6-4 timeDate_4032.109 ## [67] iterators_1.0.14 hardhat_1.4.0 lava_1.8.0 ## [70] statmod_1.5.0 ipred_0.9-15 nlme_3.1-164 ## [73] bit64_4.0.5 progress_1.2.3 filelock_1.0.3 ## [76] GenomeInfoDb_1.41.1 bslib_0.8.0 KernSmooth_2.23-24 ## [79] rpart_4.1.23 colorspace_2.1-1 BiocGenerics_0.52.0 ## [82] DBI_1.2.3 Hmisc_5.1-3 nnet_7.3-19 ## [85] tidyselect_1.2.1 bit_4.0.5 compiler_4.4.1 ## [88] curl_5.2.1 httr2_1.0.2 htmlTable_2.4.3 ## [91] xml2_1.3.6 plotly_4.10.4 stringfish_0.16.0 ## [94] bookdown_0.40 checkmate_2.3.1 scales_1.3.0 ## [97] caTools_1.18.2 rappdirs_0.3.3 digest_0.6.36 ## [100] rmarkdown_2.27 XVector_0.44.0 htmltools_0.5.8.1 ## [103] pkgconfig_2.0.3 base64enc_0.1-3 highr_0.11 ## [106] dbplyr_2.5.0 fastmap_1.2.0 rlang_1.1.4 ## [109] GlobalOptions_0.1.2 htmlwidgets_1.6.4 UCSC.utils_1.1.0 ## [112] farver_2.1.2 jquerylib_0.1.4 jsonlite_1.8.8 ## [115] BiocParallel_1.39.0 ModelMetrics_1.2.2.2 magrittr_2.0.3 ## [118] Formula_1.2-5 GenomeInfoDbData_1.2.12 dotCall64_1.1-1 ## [121] munsell_0.5.1 Rcpp_1.0.13 stringi_1.8.4 ## [124] zlibbioc_1.50.0 MASS_7.3-60.2 parallel_4.4.1 ## [127] listenv_0.9.1 Biostrings_2.72.1 splines_4.4.1 ## [130] multtest_2.61.0 hms_1.1.3 locfit_1.5-9.10 ## [133] igraph_2.0.3 reshape2_1.4.4 stats4_4.4.1 ## [136] evaluate_0.24.0 RcppParallel_5.1.8 tzdb_0.4.0 ## [139] foreach_1.5.2 qs_0.26.3 future_1.33.2 ## [142] clue_0.3-65 e1071_1.7-14 glasso_1.11 ## [145] class_7.3-22 ragg_1.3.2 AnnotationDbi_1.67.0 ## [148] ellipse_0.5.0 IRanges_2.38.1 cluster_2.1.6 ## [151] timechange_0.3.0 globals_0.16.3 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
